{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to NDD docs # NDD is an opensource Kubernetes add-on that enables platform and application teams to consume network devices in a similar way as other resources are consumed in Kubernetes . NDD uses a modular approach, through providers, which allows multiple network device types to be supported. NDD allows the network providers to be generated from YANG data models , which enables rapid enablement of multiple network device types. Through YANG we can provider automate input and dependency management between the various resource that are consumed within the device. An NDD provider represents the device model through various Customer Resources within the Kubernetes API in order to provide flexible management of the device resources. NDD is build on the basis of the kubebuilder and operator pattern within kubernetes. Features: Device discovery and Provider registration Declaritive CRUD configuration of network devices through Customer Resources Configuration Input Validation: Declarative validation using an OpenAPI v3 schema derived from YANG Runtime Dependency Management amongst the various resources comsumed within a device (parent dependency management and leaf reference dependency management amont resources) Automatic or Operator interacted configuration drift management Delete Policy, and Active etc","title":"Home"},{"location":"#welcome-to-ndd-docs","text":"NDD is an opensource Kubernetes add-on that enables platform and application teams to consume network devices in a similar way as other resources are consumed in Kubernetes . NDD uses a modular approach, through providers, which allows multiple network device types to be supported. NDD allows the network providers to be generated from YANG data models , which enables rapid enablement of multiple network device types. Through YANG we can provider automate input and dependency management between the various resource that are consumed within the device. An NDD provider represents the device model through various Customer Resources within the Kubernetes API in order to provide flexible management of the device resources. NDD is build on the basis of the kubebuilder and operator pattern within kubernetes. Features: Device discovery and Provider registration Declaritive CRUD configuration of network devices through Customer Resources Configuration Input Validation: Declarative validation using an OpenAPI v3 schema derived from YANG Runtime Dependency Management amongst the various resources comsumed within a device (parent dependency management and leaf reference dependency management amont resources) Automatic or Operator interacted configuration drift management Delete Policy, and Active etc","title":"Welcome to NDD docs"},{"location":"api/overview/","text":"API Documentation # The ndd ecosystem contains many CRDs that map to API types represented by external network device providers. The documentation for these CRDs are auto-generated on doc.crds.dev . To find the CRDs available for providers maintained by the ndd team, you can search for the Github URL, or append it in the doc.crds.dev URL path. For instance, to find the CRDs available for ndd-provider-srl , you would go to: doc.crds.dev/github.com/yndd/ndd-provider-srl By default, you will be served the latest CRDs on the master branch for the repository. If you prefer to see the CRDs for a specific version, you can append the git tag for the release: doc.crds.dev/github.com/yndd/ndd-provider-srl@v0.1.4 Ndd repositories that are not providers but do publish CRDs are also served on doc.crds.dev . For instance, the yndd/ndd-core repository. Bugs and feature requests for API documentation should be opened as issues on the open source doc.crds.dev repo .","title":"overview"},{"location":"api/overview/#api-documentation","text":"The ndd ecosystem contains many CRDs that map to API types represented by external network device providers. The documentation for these CRDs are auto-generated on doc.crds.dev . To find the CRDs available for providers maintained by the ndd team, you can search for the Github URL, or append it in the doc.crds.dev URL path. For instance, to find the CRDs available for ndd-provider-srl , you would go to: doc.crds.dev/github.com/yndd/ndd-provider-srl By default, you will be served the latest CRDs on the master branch for the repository. If you prefer to see the CRDs for a specific version, you can append the git tag for the release: doc.crds.dev/github.com/yndd/ndd-provider-srl@v0.1.4 Ndd repositories that are not providers but do publish CRDs are also served on doc.crds.dev . For instance, the yndd/ndd-core repository. Bugs and feature requests for API documentation should be opened as issues on the open source doc.crds.dev repo .","title":"API Documentation"},{"location":"community/community/","text":"Everybody is welcome to join and chat to our community members about all things ndd! Join Ndd Discord Server","title":"Community"},{"location":"concepts/architecture/","text":"","title":"Architecture"},{"location":"concepts/package/","text":"","title":"Packages"},{"location":"concepts/provider/","text":"","title":"Providers"},{"location":"concepts/resource/","text":"","title":"Managed Resource"},{"location":"concepts/terminology/","text":"","title":"Terminology"},{"location":"examples/examples/","text":"","title":"Examples"},{"location":"faq/faq/","text":"","title":"FAQ"},{"location":"start/install/","text":"Installation # NDD is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of network devices. Pre-requisites # Install a Kubernetest Cluster # Install a kubernetes cluster based on your preferences. Some examples are provided here for reference, but if you would use another kubernetes cluster flavor you can skip this step. kind cluster Install the kind sw curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind Create the kind cluster kind create cluster --name ndd Check if the cluster is running kubectl get node Expected output or similar NAME STATUS ROLES AGE VERSION ndd-control-plane Ready master 2m58s v1.19.1 The full documentation for installing a kind can be found here: kind installation minikube cluster minikube getting started can be found here: https://minikube.sigs.k8s.io/docs/start/ Install a network device # Install a network device based on your preference. You can use container based labs, real HW or other options. Some examples are provided here for reference. if you have access to a network device through other mean you can skip this step. Also you can enable multiple networking devices. containerlab containerlab is a tool that deploys container based meshed lab topologies. Various vendors are supported and extensive documentation is provided here: containerlab In this example we use SRL for which containerlab sets up gnmi by default real HW Real HW installation should be looked up based on the respective vendor that is used E.g. gnmi will be used and should be configured Enable connectivity from the cluster to the network device # NDD is using outbound communication from within the kubernetes cluster to the network device. In some environments some special action need to be taken kind cluster with container lab If you run containerlab on the same linux machine and when both the kind cluster and containerlab uses seperate bridges, you should add the following rules in iptables to allow communication between the kind cluster network and container lab management network. The bridges are specific to your environment sudo iptables -I FORWARD 1 -i br-7ee0d52a222c -o br-1f1c85a8b4c5 -j ACCEPT sudo iptables -I FORWARD 1 -o br-7ee0d52a222c -i br-1f1c85a8b4c5 -j ACCEPT Install ndd-core # ndd-core provides the core of the ndd project. ndd-core enables: the installation, configuration and life cycle management of network device drivers the installation, configuration and life cycle management of ndd providers rbac for the ndd providers in the cluster latest kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml stable kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml After this step a ndd-system namespace is created, the ndd-core crds are created and 2 pods are running in ndd-system namespace. Namespace kubectl get ns NAME STATUS AGE default Active 11m kube-node-lease Active 11m kube-public Active 11m kube-system Active 11m local-path-storage Active 11m ndd-system Active 2m6s CRDs: kubectl get crd NAME CREATED AT controllerconfigs.pkg.ndd.yndd.io 2021-09-05T11:34:20Z devicedrivers.dvr.ndd.yndd.io 2021-09-05T11:34:20Z locks.pkg.ndd.yndd.io 2021-09-05T11:34:20Z networknodes.dvr.ndd.yndd.io 2021-09-05T11:34:20Z networknodeusages.dvr.ndd.yndd.io 2021-09-05T11:34:20Z providerrevisions.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.meta.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.pkg.ndd.yndd.io 2021-09-05T11:34:20Z PODs: kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 7m16s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 7m16s When all of this succeeded we can starrt provisioning network devices through kubernetes","title":"Install ndd"},{"location":"start/install/#installation","text":"NDD is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of network devices.","title":"Installation"},{"location":"start/install/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"start/install/#install-a-kubernetest-cluster","text":"Install a kubernetes cluster based on your preferences. Some examples are provided here for reference, but if you would use another kubernetes cluster flavor you can skip this step. kind cluster Install the kind sw curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind Create the kind cluster kind create cluster --name ndd Check if the cluster is running kubectl get node Expected output or similar NAME STATUS ROLES AGE VERSION ndd-control-plane Ready master 2m58s v1.19.1 The full documentation for installing a kind can be found here: kind installation minikube cluster minikube getting started can be found here: https://minikube.sigs.k8s.io/docs/start/","title":"Install a Kubernetest Cluster"},{"location":"start/install/#install-a-network-device","text":"Install a network device based on your preference. You can use container based labs, real HW or other options. Some examples are provided here for reference. if you have access to a network device through other mean you can skip this step. Also you can enable multiple networking devices. containerlab containerlab is a tool that deploys container based meshed lab topologies. Various vendors are supported and extensive documentation is provided here: containerlab In this example we use SRL for which containerlab sets up gnmi by default real HW Real HW installation should be looked up based on the respective vendor that is used E.g. gnmi will be used and should be configured","title":"Install a network device"},{"location":"start/install/#enable-connectivity-from-the-cluster-to-the-network-device","text":"NDD is using outbound communication from within the kubernetes cluster to the network device. In some environments some special action need to be taken kind cluster with container lab If you run containerlab on the same linux machine and when both the kind cluster and containerlab uses seperate bridges, you should add the following rules in iptables to allow communication between the kind cluster network and container lab management network. The bridges are specific to your environment sudo iptables -I FORWARD 1 -i br-7ee0d52a222c -o br-1f1c85a8b4c5 -j ACCEPT sudo iptables -I FORWARD 1 -o br-7ee0d52a222c -i br-1f1c85a8b4c5 -j ACCEPT","title":"Enable connectivity from the cluster to the network device"},{"location":"start/install/#install-ndd-core","text":"ndd-core provides the core of the ndd project. ndd-core enables: the installation, configuration and life cycle management of network device drivers the installation, configuration and life cycle management of ndd providers rbac for the ndd providers in the cluster latest kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml stable kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml After this step a ndd-system namespace is created, the ndd-core crds are created and 2 pods are running in ndd-system namespace. Namespace kubectl get ns NAME STATUS AGE default Active 11m kube-node-lease Active 11m kube-public Active 11m kube-system Active 11m local-path-storage Active 11m ndd-system Active 2m6s CRDs: kubectl get crd NAME CREATED AT controllerconfigs.pkg.ndd.yndd.io 2021-09-05T11:34:20Z devicedrivers.dvr.ndd.yndd.io 2021-09-05T11:34:20Z locks.pkg.ndd.yndd.io 2021-09-05T11:34:20Z networknodes.dvr.ndd.yndd.io 2021-09-05T11:34:20Z networknodeusages.dvr.ndd.yndd.io 2021-09-05T11:34:20Z providerrevisions.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.meta.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.pkg.ndd.yndd.io 2021-09-05T11:34:20Z PODs: kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 7m16s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 7m16s When all of this succeeded we can starrt provisioning network devices through kubernetes","title":"Install ndd-core"},{"location":"start/provision/","text":"Provision # NDD allow to provision a network device through kubernetes using a network node device (nn) driver and a ndd-provider. The network node device driver interacts with the device through GNMI and provides a caching layer between the network device and the provider. The ndd-provider installs the configuration parameters in a declaritive way to the network device, through the network node device driver. Setup a network node device driver # To setup the network node device driver we first need to configure a network node and optionally device driver parameters. Setup a secret # A secret is used to connect to the network device through gnmi. Create a nddsrlsecret.yaml file with the base64 encoded username and password file: nddsrlsecret.yaml --- apiVersion: v1 kind: Secret metadata: name: srl-secrets type: Opaque data: username: YWRtaW4K password: YWRtaW4K apply the secret to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created Create a network node file leaf1.yaml: in the credentialsName: refer to the name of the secret created in the previous step in the address: supply the IP address of the network node that was setup in the network node installation step. In this Example we use gnmi between the network device driver and the network device, so also the gnmi port is relevant in the address section. file leaf1.yaml --- apiVersion: dvr.ndd.yndd.io/v1 kind: NetworkNode metadata: name: leaf1 labels: target-group: leaf-grp1 target: leaf1 spec: # deviceDriverKind: gnmi target: address: 172.20.20.5:57400 credentialsName: srl-secrets encoding: JSON_IETF skpVerify: true apply the network node configuration to the cluster kubectl apply -f leaf1.yaml networknode.dvr.ndd.yndd.io/leaf1 unchanged After the creation of the network node we should see: a new pod appear in the cluster which is providing the caching function between the network device and the ndd-provider the network node state should be HEALTHY: True, CONFIGURED: True, READY: False. READY state is false because the provider is not yet installed and registered to the network node device driver. Pods in ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 38m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 17m ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 38m Network node status: kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True False 172.20.20.5:57400 gnmi 9999 19m Install the Provider # Install a provider which exposes the network device configuration through the kubernetes API. Create a nddsrlpackage.yaml file, which provides: - name of the provider: ndd-provider-srl - package name: yndd/ndd-provider-srl:latest file nddsrlpackage.yaml apiVersion: pkg.ndd.yndd.io/v1 kind: Provider metadata: name: ndd-provider-srl namespace: ndd-system spec: package: yndd/ndd-provider-srl:latest packagePullPolicy: Always apply the provider to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created to see the installation of the provider you can use: watch kubectl get pkg After this step is successfull you should see the provider pod running in the ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 51m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 30m ndd-provider-srl-67b9e61445f6-846cb5c64c-t2w9h 1/1 Running 0 2m35s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 51m Also you should see a number of CRDs being installed in the cluster which are owned by the provider you installed. kubectl get crd | grep srl registrations.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlbfds.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlinterfacesubinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceaggregateroutes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstancenexthopgroups.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgpevpns.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgps.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgpvpns.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsises.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolslinuxes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsospfs.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstances.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstancestaticroutes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicyaspathsets.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicycommunitysets.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicypolicies.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlroutingpolicyprefixsets.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemmtus.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnames.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsbgpvpns.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstanceesis.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstances.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpns.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemntps.srl.ndd.yndd.io 2021-09-05T12:23:02Z srltunnelinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:02Z srltunnelinterfacevxlaninterfaces.srl.ndd.yndd.io 2021-09-05T12:23:02Z When the provider and the network device type matches you should see the network node becoming in ready status and you should see that the network node discovered additional parameters of the network device, such as sw version ,type, etc kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True True 172.20.20.5:57400 gnmi nokia-srl 7220 IXR-D2 v21.3.1 02:BF:A9:FF:00:00 Sim Serial No. 9999 47m Provision a NDD managed resource # Create a managed resource for the provider installed. E.g. an interface and subinterface. These resources are specific for the ndd provider you use. A ndd managed resource has the following attributes: - Kubernetes specific parameters: - A apiversion: enabled by the provider - A Kind: enabled by the provider - Metadata: name, namespace, label and annotations - Spec specifics for ndd: - networkNodeRef: refers to the network node name that was created in the previous step - forNetworkNode: refers to the specifics of the managed resource CRD enabled by the provider: see provider API documentation for more details. file int-e1-49.yaml apiVersion: srl.ndd.yndd.io/v1 kind: SrlInterface metadata: name: int-e1-49 namespace: default spec: active: true networkNodeRef: name: leaf1 forNetworkNode: interface: name: \"ethernet-1/49\" admin-state: \"enable\" description: \"ndd-ethernet-1/49\" vlan-tagging: true kubectl apply -f int-e1-49.yaml file subint-e1-49-0.yaml apiVersion: srl.ndd.yndd.io/v1 kind: SrlInterfaceSubinterface metadata: name: subint-e1-49-0 namespace: default spec: active: true networkNodeRef: name: leaf1 forNetworkNode: interface-name: ethernet-1/49 subinterface: index: 1 type: routed admin-state: enable description: \"ndd-e1-49-0-leaf1\" ipv4: address: - ip-prefix: 100.64.0.0/31 ipv6: address: - ip-prefix: 3100:64::/127 vlan: encap: single-tagged: vlan-id: \"1\" kubectl apply -f subint-e1-49-0.yaml When loggind in to the node you should see the configuration that was supplied via the ndd-provider in the network device info from running / interface ethernet-1/49 interface ethernet-1/49 { description ndd-ethernet-1/49 admin-state enable vlan-tagging true subinterface 1 { type routed description ndd-e1-49-0-leaf1 admin-state enable ipv4 { allow-directed-broadcast false address 100.64.0.0/31 { } } ipv6 { address 3100:64::/127 { } } vlan { encap { single-tagged { vlan-id 1 } } } } }","title":"Provision a network device"},{"location":"start/provision/#provision","text":"NDD allow to provision a network device through kubernetes using a network node device (nn) driver and a ndd-provider. The network node device driver interacts with the device through GNMI and provides a caching layer between the network device and the provider. The ndd-provider installs the configuration parameters in a declaritive way to the network device, through the network node device driver.","title":"Provision"},{"location":"start/provision/#setup-a-network-node-device-driver","text":"To setup the network node device driver we first need to configure a network node and optionally device driver parameters.","title":"Setup a network node device driver"},{"location":"start/provision/#setup-a-secret","text":"A secret is used to connect to the network device through gnmi. Create a nddsrlsecret.yaml file with the base64 encoded username and password file: nddsrlsecret.yaml --- apiVersion: v1 kind: Secret metadata: name: srl-secrets type: Opaque data: username: YWRtaW4K password: YWRtaW4K apply the secret to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created Create a network node file leaf1.yaml: in the credentialsName: refer to the name of the secret created in the previous step in the address: supply the IP address of the network node that was setup in the network node installation step. In this Example we use gnmi between the network device driver and the network device, so also the gnmi port is relevant in the address section. file leaf1.yaml --- apiVersion: dvr.ndd.yndd.io/v1 kind: NetworkNode metadata: name: leaf1 labels: target-group: leaf-grp1 target: leaf1 spec: # deviceDriverKind: gnmi target: address: 172.20.20.5:57400 credentialsName: srl-secrets encoding: JSON_IETF skpVerify: true apply the network node configuration to the cluster kubectl apply -f leaf1.yaml networknode.dvr.ndd.yndd.io/leaf1 unchanged After the creation of the network node we should see: a new pod appear in the cluster which is providing the caching function between the network device and the ndd-provider the network node state should be HEALTHY: True, CONFIGURED: True, READY: False. READY state is false because the provider is not yet installed and registered to the network node device driver. Pods in ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 38m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 17m ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 38m Network node status: kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True False 172.20.20.5:57400 gnmi 9999 19m","title":"Setup a secret"},{"location":"start/provision/#install-the-provider","text":"Install a provider which exposes the network device configuration through the kubernetes API. Create a nddsrlpackage.yaml file, which provides: - name of the provider: ndd-provider-srl - package name: yndd/ndd-provider-srl:latest file nddsrlpackage.yaml apiVersion: pkg.ndd.yndd.io/v1 kind: Provider metadata: name: ndd-provider-srl namespace: ndd-system spec: package: yndd/ndd-provider-srl:latest packagePullPolicy: Always apply the provider to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created to see the installation of the provider you can use: watch kubectl get pkg After this step is successfull you should see the provider pod running in the ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 51m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 30m ndd-provider-srl-67b9e61445f6-846cb5c64c-t2w9h 1/1 Running 0 2m35s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 51m Also you should see a number of CRDs being installed in the cluster which are owned by the provider you installed. kubectl get crd | grep srl registrations.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlbfds.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlinterfacesubinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceaggregateroutes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstancenexthopgroups.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgpevpns.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgps.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgpvpns.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsises.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolslinuxes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsospfs.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstances.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstancestaticroutes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicyaspathsets.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicycommunitysets.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicypolicies.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlroutingpolicyprefixsets.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemmtus.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnames.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsbgpvpns.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstanceesis.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstances.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpns.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemntps.srl.ndd.yndd.io 2021-09-05T12:23:02Z srltunnelinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:02Z srltunnelinterfacevxlaninterfaces.srl.ndd.yndd.io 2021-09-05T12:23:02Z When the provider and the network device type matches you should see the network node becoming in ready status and you should see that the network node discovered additional parameters of the network device, such as sw version ,type, etc kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True True 172.20.20.5:57400 gnmi nokia-srl 7220 IXR-D2 v21.3.1 02:BF:A9:FF:00:00 Sim Serial No. 9999 47m","title":"Install the Provider"},{"location":"start/provision/#provision-a-ndd-managed-resource","text":"Create a managed resource for the provider installed. E.g. an interface and subinterface. These resources are specific for the ndd provider you use. A ndd managed resource has the following attributes: - Kubernetes specific parameters: - A apiversion: enabled by the provider - A Kind: enabled by the provider - Metadata: name, namespace, label and annotations - Spec specifics for ndd: - networkNodeRef: refers to the network node name that was created in the previous step - forNetworkNode: refers to the specifics of the managed resource CRD enabled by the provider: see provider API documentation for more details. file int-e1-49.yaml apiVersion: srl.ndd.yndd.io/v1 kind: SrlInterface metadata: name: int-e1-49 namespace: default spec: active: true networkNodeRef: name: leaf1 forNetworkNode: interface: name: \"ethernet-1/49\" admin-state: \"enable\" description: \"ndd-ethernet-1/49\" vlan-tagging: true kubectl apply -f int-e1-49.yaml file subint-e1-49-0.yaml apiVersion: srl.ndd.yndd.io/v1 kind: SrlInterfaceSubinterface metadata: name: subint-e1-49-0 namespace: default spec: active: true networkNodeRef: name: leaf1 forNetworkNode: interface-name: ethernet-1/49 subinterface: index: 1 type: routed admin-state: enable description: \"ndd-e1-49-0-leaf1\" ipv4: address: - ip-prefix: 100.64.0.0/31 ipv6: address: - ip-prefix: 3100:64::/127 vlan: encap: single-tagged: vlan-id: \"1\" kubectl apply -f subint-e1-49-0.yaml When loggind in to the node you should see the configuration that was supplied via the ndd-provider in the network device info from running / interface ethernet-1/49 interface ethernet-1/49 { description ndd-ethernet-1/49 admin-state enable vlan-tagging true subinterface 1 { type routed description ndd-e1-49-0-leaf1 admin-state enable ipv4 { allow-directed-broadcast false address 100.64.0.0/31 { } } ipv6 { address 3100:64::/127 { } } vlan { encap { single-tagged { vlan-id 1 } } } } }","title":"Provision a NDD managed resource"},{"location":"start/uninstall/","text":"","title":"Uninstall ndd"}]}