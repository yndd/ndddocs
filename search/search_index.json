{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to NDD docs # NDD is an opensource Kubernetes add-on that enables platform and application teams to consume network devices in a similar way as other resources are consumed in Kubernetes . NDD uses a modular approach, through providers, which allows multiple network device types to be supported. NDD allows the network providers to be generated from YANG data models , which enables rapid enablement of multiple network device types. Through YANG we can provider automate input and dependency management between the various resource that are consumed within the device. An NDD provider represents the device model through various Customer Resources within the Kubernetes API in order to provide flexible management of the device resources. NDD is build on the basis of the kubebuilder and operator pattern within kubernetes. Features: Device discovery and Provider registration Declaritive CRUD configuration of network devices through Customer Resources Configuration Input Validation: Declarative validation using an OpenAPI v3 schema derived from YANG Runtime Dependency Management amongst the various resources comsumed within a device (parent dependency management and leaf reference dependency management amont resources) Automatic or Operator interacted configuration drift management Delete Policy, and Active etc","title":"Home"},{"location":"#welcome-to-ndd-docs","text":"NDD is an opensource Kubernetes add-on that enables platform and application teams to consume network devices in a similar way as other resources are consumed in Kubernetes . NDD uses a modular approach, through providers, which allows multiple network device types to be supported. NDD allows the network providers to be generated from YANG data models , which enables rapid enablement of multiple network device types. Through YANG we can provider automate input and dependency management between the various resource that are consumed within the device. An NDD provider represents the device model through various Customer Resources within the Kubernetes API in order to provide flexible management of the device resources. NDD is build on the basis of the kubebuilder and operator pattern within kubernetes. Features: Device discovery and Provider registration Declaritive CRUD configuration of network devices through Customer Resources Configuration Input Validation: Declarative validation using an OpenAPI v3 schema derived from YANG Runtime Dependency Management amongst the various resources comsumed within a device (parent dependency management and leaf reference dependency management amont resources) Automatic or Operator interacted configuration drift management Delete Policy, and Active etc","title":"Welcome to NDD docs"},{"location":"api/overview/","text":"API Documentation # The ndd ecosystem contains many CRDs that map to API types represented by external network device providers. The documentation for these CRDs are auto-generated on doc.crds.dev . To find the CRDs available for providers maintained by the ndd team, you can search for the Github URL, or append it in the doc.crds.dev URL path. For instance, to find the CRDs available for ndd-provider-srl , you would go to: doc.crds.dev/github.com/yndd/ndd-provider-srl By default, you will be served the latest CRDs on the master branch for the repository. If you prefer to see the CRDs for a specific version, you can append the git tag for the release: doc.crds.dev/github.com/yndd/ndd-provider-srl@v0.1.4 Ndd repositories that are not providers but do publish CRDs are also served on doc.crds.dev . For instance, the yndd/ndd-core repository. Bugs and feature requests for API documentation should be opened as issues on the open source doc.crds.dev repo .","title":"overview"},{"location":"api/overview/#api-documentation","text":"The ndd ecosystem contains many CRDs that map to API types represented by external network device providers. The documentation for these CRDs are auto-generated on doc.crds.dev . To find the CRDs available for providers maintained by the ndd team, you can search for the Github URL, or append it in the doc.crds.dev URL path. For instance, to find the CRDs available for ndd-provider-srl , you would go to: doc.crds.dev/github.com/yndd/ndd-provider-srl By default, you will be served the latest CRDs on the master branch for the repository. If you prefer to see the CRDs for a specific version, you can append the git tag for the release: doc.crds.dev/github.com/yndd/ndd-provider-srl@v0.1.4 Ndd repositories that are not providers but do publish CRDs are also served on doc.crds.dev . For instance, the yndd/ndd-core repository. Bugs and feature requests for API documentation should be opened as issues on the open source doc.crds.dev repo .","title":"API Documentation"},{"location":"community/community/","text":"Everybody is welcome to join and chat to our community members about all things ndd! Join Ndd Discord Server","title":"Community"},{"location":"concepts/architecture/","text":"","title":"Architecture"},{"location":"concepts/package/","text":"","title":"Packages"},{"location":"concepts/provider/","text":"","title":"Providers"},{"location":"concepts/resource/","text":"","title":"Managed Resource"},{"location":"concepts/terminology/","text":"","title":"Terminology"},{"location":"examples/examples/","text":"","title":"Examples"},{"location":"faq/faq/","text":"","title":"FAQ"},{"location":"start/install-nddo/","text":"Installation # NDDO is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of networking. Pre-requisites # install ndd-core Install nddo registries # There are 5 registries right now org-registry topo-registry ipam-registry ni-registry as-registry org-registry # create a file nddr-package-org-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-org-registry namespace: ndd-system spec: package: yndd/nddr-org-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-org-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-org-registry-ac199de081fe-5bf5bc7f96-c8sph 2/2 Running 0 12s topo-registry # create a file nddr-package-topo-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-topo-registry namespace: ndd-system spec: package: yndd/nddr-topo-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-topo-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-topo-registry-83d75d175ab9-fbcd55666-k9jtg 2/2 Running 0 7s ni-registry # create a file nddr-package-ni-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-ni-registry namespace: ndd-system spec: package: yndd/nddr-ni-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-ni-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-ni-registry-4a71a2c372d0-74589448f6-nw8hl 2/2 Running 0 7s as-registry # create a file nddr-package-as-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-as-registry namespace: ndd-system spec: package: yndd/nddr-as-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-as-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-as-registry-5d7fa458ac98-544588b5cf-66w2z 2/2 Running 0 6s ipam-registry # create a file nddr-package-ipam-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-ipam-registry namespace: ndd-system spec: package: yndd/nddr-ipam-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-ipam-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-as-registry-5d7fa458ac98-544588b5cf-66w2z 2/2 Running 0 6s Install nddo adaptors # install the adaptation layer which is used by the Intents to create the abstract data structures ndda-network # create a file ndda-package-network.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: ndda-network namespace: ndd-system spec: package: yndd/ndda-adaptor-network:latest packagePullPolicy: Always kubectl apply -f ndda-package-network.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... ndda-network-0f7367906ec0-6f84898f65-8f8c8 2/2 Running 0 6s Install nddo Intents # Install the intents, there are 2 right now: - infrastructure - Endpoint groups - Vpc nddo-infrastructure # create a file nddo-package-infra.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-infra namespace: ndd-system spec: package: yndd/nddo-intent-infra:latest packagePullPolicy: Always kubectl apply -f nddo-package-infra.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-infra-0f86fd936790-5bbdbc9f85-l7tgp 2/2 Running 0 6s nddo-epgroup # create a file nddo-package-epgroup.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-epgroup namespace: ndd-system spec: package: yndd/nddo-intent-epgroup:latest packagePullPolicy: Always kubectl apply -f nddo-package-epgroup.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-infra-0f86fd936790-5bbdbc9f85-l7tgp 2/2 Running 0 6s","title":"Install nddo"},{"location":"start/install-nddo/#installation","text":"NDDO is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of networking.","title":"Installation"},{"location":"start/install-nddo/#pre-requisites","text":"install ndd-core","title":"Pre-requisites"},{"location":"start/install-nddo/#install-nddo-registries","text":"There are 5 registries right now org-registry topo-registry ipam-registry ni-registry as-registry","title":"Install nddo registries"},{"location":"start/install-nddo/#org-registry","text":"create a file nddr-package-org-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-org-registry namespace: ndd-system spec: package: yndd/nddr-org-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-org-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-org-registry-ac199de081fe-5bf5bc7f96-c8sph 2/2 Running 0 12s","title":"org-registry"},{"location":"start/install-nddo/#topo-registry","text":"create a file nddr-package-topo-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-topo-registry namespace: ndd-system spec: package: yndd/nddr-topo-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-topo-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-topo-registry-83d75d175ab9-fbcd55666-k9jtg 2/2 Running 0 7s","title":"topo-registry"},{"location":"start/install-nddo/#ni-registry","text":"create a file nddr-package-ni-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-ni-registry namespace: ndd-system spec: package: yndd/nddr-ni-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-ni-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-ni-registry-4a71a2c372d0-74589448f6-nw8hl 2/2 Running 0 7s","title":"ni-registry"},{"location":"start/install-nddo/#as-registry","text":"create a file nddr-package-as-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-as-registry namespace: ndd-system spec: package: yndd/nddr-as-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-as-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-as-registry-5d7fa458ac98-544588b5cf-66w2z 2/2 Running 0 6s","title":"as-registry"},{"location":"start/install-nddo/#ipam-registry","text":"create a file nddr-package-ipam-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-ipam-registry namespace: ndd-system spec: package: yndd/nddr-ipam-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-ipam-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-as-registry-5d7fa458ac98-544588b5cf-66w2z 2/2 Running 0 6s","title":"ipam-registry"},{"location":"start/install-nddo/#install-nddo-adaptors","text":"install the adaptation layer which is used by the Intents to create the abstract data structures","title":"Install nddo adaptors"},{"location":"start/install-nddo/#ndda-network","text":"create a file ndda-package-network.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: ndda-network namespace: ndd-system spec: package: yndd/ndda-adaptor-network:latest packagePullPolicy: Always kubectl apply -f ndda-package-network.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... ndda-network-0f7367906ec0-6f84898f65-8f8c8 2/2 Running 0 6s","title":"ndda-network"},{"location":"start/install-nddo/#install-nddo-intents","text":"Install the intents, there are 2 right now: - infrastructure - Endpoint groups - Vpc","title":"Install nddo Intents"},{"location":"start/install-nddo/#nddo-infrastructure","text":"create a file nddo-package-infra.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-infra namespace: ndd-system spec: package: yndd/nddo-intent-infra:latest packagePullPolicy: Always kubectl apply -f nddo-package-infra.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-infra-0f86fd936790-5bbdbc9f85-l7tgp 2/2 Running 0 6s","title":"nddo-infrastructure"},{"location":"start/install-nddo/#nddo-epgroup","text":"create a file nddo-package-epgroup.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-epgroup namespace: ndd-system spec: package: yndd/nddo-intent-epgroup:latest packagePullPolicy: Always kubectl apply -f nddo-package-epgroup.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-infra-0f86fd936790-5bbdbc9f85-l7tgp 2/2 Running 0 6s","title":"nddo-epgroup"},{"location":"start/install/","text":"Installation # NDD is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of network devices. Pre-requisites # Install a Kubernetest Cluster # Install a kubernetes cluster based on your preferences. Some examples are provided here for reference, but if you would use another kubernetes cluster flavor you can skip this step. kind cluster Install the kind sw curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind Create the kind cluster kind create cluster --name ndd Check if the cluster is running kubectl get node Expected output or similar NAME STATUS ROLES AGE VERSION ndd-control-plane Ready master 2m58s v1.19.1 The full documentation for installing a kind can be found here: kind installation minikube cluster minikube getting started can be found here: https://minikube.sigs.k8s.io/docs/start/ Install a network device # Install a network device based on your preference. You can use container based labs, real HW or other options. Some examples are provided here for reference. if you have access to a network device through other mean you can skip this step. Also you can enable multiple networking devices. containerlab containerlab is a tool that deploys container based meshed lab topologies. Various vendors are supported and extensive documentation is provided here: containerlab In this example we use SRL for which containerlab sets up gnmi by default real HW Real HW installation should be looked up based on the respective vendor that is used E.g. gnmi will be used and should be configured Enable connectivity from the cluster to the network device # NDD is using outbound communication from within the kubernetes cluster to the network device. In some environments some special action need to be taken kind cluster with container lab If you run containerlab on the same linux machine and when both the kind cluster and containerlab uses seperate bridges, you should add the following rules in iptables to allow communication between the kind cluster network and container lab management network. The bridges are specific to your environment sudo iptables -I FORWARD 1 -i br-7ee0d52a222c -o br-1f1c85a8b4c5 -j ACCEPT sudo iptables -I FORWARD 1 -o br-7ee0d52a222c -i br-1f1c85a8b4c5 -j ACCEPT Install ndd-core # ndd-core provides the core of the ndd project. ndd-core enables: the installation, configuration and life cycle management of network device drivers the installation, configuration and life cycle management of ndd providers rbac for the ndd providers in the cluster latest kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml stable kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml After this step a ndd-system namespace is created, the ndd-core crds are created and 2 pods are running in ndd-system namespace. Namespace kubectl get ns NAME STATUS AGE default Active 11m kube-node-lease Active 11m kube-public Active 11m kube-system Active 11m local-path-storage Active 11m ndd-system Active 2m6s CRDs: kubectl get crd NAME CREATED AT controllerconfigs.pkg.ndd.yndd.io 2021-09-05T11:34:20Z devicedrivers.dvr.ndd.yndd.io 2021-09-05T11:34:20Z locks.pkg.ndd.yndd.io 2021-09-05T11:34:20Z networknodes.dvr.ndd.yndd.io 2021-09-05T11:34:20Z networknodeusages.dvr.ndd.yndd.io 2021-09-05T11:34:20Z providerrevisions.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.meta.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.pkg.ndd.yndd.io 2021-09-05T11:34:20Z PODs: kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 7m16s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 7m16s When all of this succeeded we can starrt provisioning network devices through kubernetes","title":"Install ndd"},{"location":"start/install/#installation","text":"NDD is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of network devices.","title":"Installation"},{"location":"start/install/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"start/install/#install-a-kubernetest-cluster","text":"Install a kubernetes cluster based on your preferences. Some examples are provided here for reference, but if you would use another kubernetes cluster flavor you can skip this step. kind cluster Install the kind sw curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind Create the kind cluster kind create cluster --name ndd Check if the cluster is running kubectl get node Expected output or similar NAME STATUS ROLES AGE VERSION ndd-control-plane Ready master 2m58s v1.19.1 The full documentation for installing a kind can be found here: kind installation minikube cluster minikube getting started can be found here: https://minikube.sigs.k8s.io/docs/start/","title":"Install a Kubernetest Cluster"},{"location":"start/install/#install-a-network-device","text":"Install a network device based on your preference. You can use container based labs, real HW or other options. Some examples are provided here for reference. if you have access to a network device through other mean you can skip this step. Also you can enable multiple networking devices. containerlab containerlab is a tool that deploys container based meshed lab topologies. Various vendors are supported and extensive documentation is provided here: containerlab In this example we use SRL for which containerlab sets up gnmi by default real HW Real HW installation should be looked up based on the respective vendor that is used E.g. gnmi will be used and should be configured","title":"Install a network device"},{"location":"start/install/#enable-connectivity-from-the-cluster-to-the-network-device","text":"NDD is using outbound communication from within the kubernetes cluster to the network device. In some environments some special action need to be taken kind cluster with container lab If you run containerlab on the same linux machine and when both the kind cluster and containerlab uses seperate bridges, you should add the following rules in iptables to allow communication between the kind cluster network and container lab management network. The bridges are specific to your environment sudo iptables -I FORWARD 1 -i br-7ee0d52a222c -o br-1f1c85a8b4c5 -j ACCEPT sudo iptables -I FORWARD 1 -o br-7ee0d52a222c -i br-1f1c85a8b4c5 -j ACCEPT","title":"Enable connectivity from the cluster to the network device"},{"location":"start/install/#install-ndd-core","text":"ndd-core provides the core of the ndd project. ndd-core enables: the installation, configuration and life cycle management of network device drivers the installation, configuration and life cycle management of ndd providers rbac for the ndd providers in the cluster latest kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml stable kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml After this step a ndd-system namespace is created, the ndd-core crds are created and 2 pods are running in ndd-system namespace. Namespace kubectl get ns NAME STATUS AGE default Active 11m kube-node-lease Active 11m kube-public Active 11m kube-system Active 11m local-path-storage Active 11m ndd-system Active 2m6s CRDs: kubectl get crd NAME CREATED AT controllerconfigs.pkg.ndd.yndd.io 2021-09-05T11:34:20Z devicedrivers.dvr.ndd.yndd.io 2021-09-05T11:34:20Z locks.pkg.ndd.yndd.io 2021-09-05T11:34:20Z networknodes.dvr.ndd.yndd.io 2021-09-05T11:34:20Z networknodeusages.dvr.ndd.yndd.io 2021-09-05T11:34:20Z providerrevisions.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.meta.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.pkg.ndd.yndd.io 2021-09-05T11:34:20Z PODs: kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 7m16s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 7m16s When all of this succeeded we can starrt provisioning network devices through kubernetes","title":"Install ndd-core"},{"location":"start/provision-nddo-intents/","text":"Provision # After the registers are setup we can configure the intents infrastructure # apiVersion: infra.nddo.yndd.io/v1alpha1 kind: Infrastructure metadata: name: nokia.region1.infra namespace: default spec: infrastructure: network-instance-name: default-routed admin-state: enable addressing-scheme: dual-stack underlay-protocol: - 'ebgp' overlay-protocol: - 'evpn' show status k get infrastructures.infra.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ NI ADDR UNDERLAY OVERLAY AGE nokia.region1.infra True True nokia region1 default-routed dual-stack ebgp evpn 23s this should result in the network instances and interfaces created in the adaptor layer k get interfaces.network.ndda.yndd.io NAME AGE nokia.region1.infrastructure.infra.leaf1-int-1-1-49-interface 94s nokia.region1.infrastructure.infra.leaf1-int-1-1-50-interface 94s nokia.region1.infrastructure.infra.leaf1-irb-irb 94s nokia.region1.infrastructure.infra.leaf1-lag-50-interface 94s nokia.region1.infrastructure.infra.leaf1-system-system 94s nokia.region1.infrastructure.infra.leaf1-vxlan-vxlan 94s nokia.region1.infrastructure.infra.leaf2-int-1-1-49-interface 94s nokia.region1.infrastructure.infra.leaf2-int-1-1-50-interface 94s nokia.region1.infrastructure.infra.leaf2-irb-irb 94s nokia.region1.infrastructure.infra.leaf2-lag-50-interface 94s nokia.region1.infrastructure.infra.leaf2-system-system 94s nokia.region1.infrastructure.infra.leaf2-vxlan-vxlan 94s k get networkinstances.network.ndda.yndd.io NAME AGE nokia.region1.infrastructure.infra.leaf1 108s nokia.region1.infrastructure.infra.leaf2 108s endpoint groups # server pods apiVersion: epg.nddo.yndd.io/v1alpha1 kind: EpGroup metadata: name: nokia.region1.server-pod1 namespace: default spec: ep-group: admin-state: enable dcgw apiVersion: epg.nddo.yndd.io/v1alpha1 kind: EpGroup metadata: name: nokia.region1.dcgw namespace: default spec: ep-group: admin-state: enable show status k get epgroups.epg.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ EPGNAME DEPL AGE nokia.region1.dcgw True True nokia region1 dcgw 72s nokia.region1.server-pod1 True True nokia region1 server-pod1 72s This shoudl result in additional interfaces created in the adaptor layer k get interfaces.network.ndda.yndd.io NAME AGE nokia.region1.epgroup.dcgw.leaf1-int-1-1-48-interface 118s nokia.region1.epgroup.dcgw.leaf2-int-1-1-48-interface 118s nokia.region1.epgroup.server-pod1.leaf1-int-1-1-1-interface 118s nokia.region1.epgroup.server-pod1.leaf1-lag-1-interface 118s nokia.region1.epgroup.server-pod1.leaf2-int-1-1-1-interface 118s nokia.region1.epgroup.server-pod1.leaf2-lag-1-interface 118s vpc # apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.prov namespace: default spec: vpc: admin-state: enable description: vpc for server provisioning bridge-domains: - name: provisioning interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} vlan: 10 apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.infra namespace: default spec: vpc: defaults: tunnel: vxlan protocol: evpn admin-state: enable description: vpc for server infrastructure bridge-domains: - name: infrastructure tunnel: vxlan protocol: evpn interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} vlan: 40 routing-tables: - name: infrastructure tunnel: vxlan protocol: evpn bridge-domains: - name: infrastructure ipv4-prefixes: [100.112.3.0/24] ipv6-prefixes: [2a02:1800:80:7000::/64] interface-selector: - tag: - {key: kind, value: node-itfce} - {key: leaf1, value: int-1/1/48} ipv4-prefixes: [100.112.10.1/31] ipv6-prefixes: [2a02:1800:80:7050::1/64] vlan: 10 - tag: - {key: kind, value: node-itfce} - {key: leaf2, value: int-1/1/48} ipv4-prefixes: [100.112.10.3/31] ipv6-prefixes: [2a02:1800:80:7051::1/64] vlan: 11 show status k get vpcs.vpc.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ VPC AGE nokia.region1.infra True True nokia region1 infra 8s nokia.region1.prov True True nokia region1 prov 13s this shoudl result in additional network instances and subinterface creation in the adaptor layer k get subinterfaces.network.ndda.yndd.io NAME AGE nokia.region1.infrastructure.infra.leaf1-lag-50-0-routed 6m57s nokia.region1.infrastructure.infra.leaf1-system-0-routed 6m57s nokia.region1.infrastructure.infra.leaf2-lag-50-0-routed 6m57s nokia.region1.infrastructure.infra.leaf2-system-0-routed 6m57s nokia.region1.vpc.infra.leaf1-int-1-1-48-10-routed 98s nokia.region1.vpc.infra.leaf1-irb-12303-routed 98s nokia.region1.vpc.infra.leaf1-irb-2303-bridged 98s nokia.region1.vpc.infra.leaf1-lag-1-40-bridged 98s nokia.region1.vpc.infra.leaf1-vxlan-2241-routed 98s nokia.region1.vpc.infra.leaf1-vxlan-2303-bridged 98s nokia.region1.vpc.infra.leaf2-int-1-1-48-11-routed 98s nokia.region1.vpc.infra.leaf2-irb-12303-routed 98s nokia.region1.vpc.infra.leaf2-irb-2303-bridged 98s nokia.region1.vpc.infra.leaf2-lag-1-40-bridged 98s nokia.region1.vpc.infra.leaf2-vxlan-2241-routed 98s nokia.region1.vpc.infra.leaf2-vxlan-2303-bridged 98s nokia.region1.vpc.prov.leaf1-lag-1-10-bridged 103s nokia.region1.vpc.prov.leaf1-vxlan-2085-bridged 103s nokia.region1.vpc.prov.leaf2-lag-1-10-bridged 103s nokia.region1.vpc.prov.leaf2-vxlan-2085-bridged 103s k get networkinstances.network.ndda.yndd.io NAME AGE nokia.region1.infrastructure.infra.leaf1 7m18s nokia.region1.infrastructure.infra.leaf2 7m18s nokia.region1.vpc.infra.leaf1 119s nokia.region1.vpc.infra.leaf2 119s nokia.region1.vpc.prov.leaf1 2m4s nokia.region1.vpc.prov.leaf2 2m4s","title":"Provision nddo intents"},{"location":"start/provision-nddo-intents/#provision","text":"After the registers are setup we can configure the intents","title":"Provision"},{"location":"start/provision-nddo-intents/#infrastructure","text":"apiVersion: infra.nddo.yndd.io/v1alpha1 kind: Infrastructure metadata: name: nokia.region1.infra namespace: default spec: infrastructure: network-instance-name: default-routed admin-state: enable addressing-scheme: dual-stack underlay-protocol: - 'ebgp' overlay-protocol: - 'evpn' show status k get infrastructures.infra.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ NI ADDR UNDERLAY OVERLAY AGE nokia.region1.infra True True nokia region1 default-routed dual-stack ebgp evpn 23s this should result in the network instances and interfaces created in the adaptor layer k get interfaces.network.ndda.yndd.io NAME AGE nokia.region1.infrastructure.infra.leaf1-int-1-1-49-interface 94s nokia.region1.infrastructure.infra.leaf1-int-1-1-50-interface 94s nokia.region1.infrastructure.infra.leaf1-irb-irb 94s nokia.region1.infrastructure.infra.leaf1-lag-50-interface 94s nokia.region1.infrastructure.infra.leaf1-system-system 94s nokia.region1.infrastructure.infra.leaf1-vxlan-vxlan 94s nokia.region1.infrastructure.infra.leaf2-int-1-1-49-interface 94s nokia.region1.infrastructure.infra.leaf2-int-1-1-50-interface 94s nokia.region1.infrastructure.infra.leaf2-irb-irb 94s nokia.region1.infrastructure.infra.leaf2-lag-50-interface 94s nokia.region1.infrastructure.infra.leaf2-system-system 94s nokia.region1.infrastructure.infra.leaf2-vxlan-vxlan 94s k get networkinstances.network.ndda.yndd.io NAME AGE nokia.region1.infrastructure.infra.leaf1 108s nokia.region1.infrastructure.infra.leaf2 108s","title":"infrastructure"},{"location":"start/provision-nddo-intents/#endpoint-groups","text":"server pods apiVersion: epg.nddo.yndd.io/v1alpha1 kind: EpGroup metadata: name: nokia.region1.server-pod1 namespace: default spec: ep-group: admin-state: enable dcgw apiVersion: epg.nddo.yndd.io/v1alpha1 kind: EpGroup metadata: name: nokia.region1.dcgw namespace: default spec: ep-group: admin-state: enable show status k get epgroups.epg.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ EPGNAME DEPL AGE nokia.region1.dcgw True True nokia region1 dcgw 72s nokia.region1.server-pod1 True True nokia region1 server-pod1 72s This shoudl result in additional interfaces created in the adaptor layer k get interfaces.network.ndda.yndd.io NAME AGE nokia.region1.epgroup.dcgw.leaf1-int-1-1-48-interface 118s nokia.region1.epgroup.dcgw.leaf2-int-1-1-48-interface 118s nokia.region1.epgroup.server-pod1.leaf1-int-1-1-1-interface 118s nokia.region1.epgroup.server-pod1.leaf1-lag-1-interface 118s nokia.region1.epgroup.server-pod1.leaf2-int-1-1-1-interface 118s nokia.region1.epgroup.server-pod1.leaf2-lag-1-interface 118s","title":"endpoint groups"},{"location":"start/provision-nddo-intents/#vpc","text":"apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.prov namespace: default spec: vpc: admin-state: enable description: vpc for server provisioning bridge-domains: - name: provisioning interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} vlan: 10 apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.infra namespace: default spec: vpc: defaults: tunnel: vxlan protocol: evpn admin-state: enable description: vpc for server infrastructure bridge-domains: - name: infrastructure tunnel: vxlan protocol: evpn interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} vlan: 40 routing-tables: - name: infrastructure tunnel: vxlan protocol: evpn bridge-domains: - name: infrastructure ipv4-prefixes: [100.112.3.0/24] ipv6-prefixes: [2a02:1800:80:7000::/64] interface-selector: - tag: - {key: kind, value: node-itfce} - {key: leaf1, value: int-1/1/48} ipv4-prefixes: [100.112.10.1/31] ipv6-prefixes: [2a02:1800:80:7050::1/64] vlan: 10 - tag: - {key: kind, value: node-itfce} - {key: leaf2, value: int-1/1/48} ipv4-prefixes: [100.112.10.3/31] ipv6-prefixes: [2a02:1800:80:7051::1/64] vlan: 11 show status k get vpcs.vpc.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ VPC AGE nokia.region1.infra True True nokia region1 infra 8s nokia.region1.prov True True nokia region1 prov 13s this shoudl result in additional network instances and subinterface creation in the adaptor layer k get subinterfaces.network.ndda.yndd.io NAME AGE nokia.region1.infrastructure.infra.leaf1-lag-50-0-routed 6m57s nokia.region1.infrastructure.infra.leaf1-system-0-routed 6m57s nokia.region1.infrastructure.infra.leaf2-lag-50-0-routed 6m57s nokia.region1.infrastructure.infra.leaf2-system-0-routed 6m57s nokia.region1.vpc.infra.leaf1-int-1-1-48-10-routed 98s nokia.region1.vpc.infra.leaf1-irb-12303-routed 98s nokia.region1.vpc.infra.leaf1-irb-2303-bridged 98s nokia.region1.vpc.infra.leaf1-lag-1-40-bridged 98s nokia.region1.vpc.infra.leaf1-vxlan-2241-routed 98s nokia.region1.vpc.infra.leaf1-vxlan-2303-bridged 98s nokia.region1.vpc.infra.leaf2-int-1-1-48-11-routed 98s nokia.region1.vpc.infra.leaf2-irb-12303-routed 98s nokia.region1.vpc.infra.leaf2-irb-2303-bridged 98s nokia.region1.vpc.infra.leaf2-lag-1-40-bridged 98s nokia.region1.vpc.infra.leaf2-vxlan-2241-routed 98s nokia.region1.vpc.infra.leaf2-vxlan-2303-bridged 98s nokia.region1.vpc.prov.leaf1-lag-1-10-bridged 103s nokia.region1.vpc.prov.leaf1-vxlan-2085-bridged 103s nokia.region1.vpc.prov.leaf2-lag-1-10-bridged 103s nokia.region1.vpc.prov.leaf2-vxlan-2085-bridged 103s k get networkinstances.network.ndda.yndd.io NAME AGE nokia.region1.infrastructure.infra.leaf1 7m18s nokia.region1.infrastructure.infra.leaf2 7m18s nokia.region1.vpc.infra.leaf1 119s nokia.region1.vpc.infra.leaf2 119s nokia.region1.vpc.prov.leaf1 2m4s nokia.region1.vpc.prov.leaf2 2m4s","title":"vpc"},{"location":"start/provision-nddo-registers/","text":"Provision # First we need to configure the registries Configure the organization registry # organization # we create an organization apiVersion: org.nddr.yndd.io/v1alpha1 kind: Organization metadata: name: nokia namespace: default spec: organization: description: default organization for Nokia register: - {kind: ipam, name: nokia-default} - {kind: ni, name: nokia-default} - {kind: as, name: nokia-default} - {kind: vlan, name: nokia-default} deployment # create a deployment apiVersion: org.nddr.yndd.io/v1alpha1 kind: Deployment metadata: name: nokia.region1 namespace: default spec: deployment: region: an Configure a topology in a deployment # fabric # apiVersion: topo.nddr.yndd.io/v1alpha1 kind: Topology metadata: name: nokia.region1.fabric1 namespace: default spec: topology: kind: - name: srl tag: - key: platform value: ixrd2 - name: sros tag: - key: platform value: sr1 - name: linux nodes # apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.leaf1 namespace: default spec: node: kind-name: srl tag: - {key: index, value: \"0\"} - {key: position, value: leaf} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.leaf2 namespace: default spec: node: kind-name: srl tag: - {key: index, value: \"1\"} - {key: position, value: leaf} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.dcgw1 namespace: default spec: node: kind-name: sros tag: - {key: position, value: dcgw} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.dcgw2 namespace: default spec: node: kind-name: sros tag: - {key: position, value: dcgw} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.master0 namespace: default spec: node: kind-name: linux tag: - {key: position, value: server} links # apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link50-leaf1-leaf2 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/50 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} - node-name: leaf2 interface-name: int-1/1/50 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link49-leaf1-leaf2 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/49 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} - node-name: leaf2 interface-name: int-1/1/49 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link48-leaf1-dcgw1 namespace: default spec: link: tag: endpoints: - node-name: leaf1 interface-name: int-1/1/48 tag: - {key: kind, value: access} - {key: endpoint-group, value: dcgw} - node-name: dcgw1 interface-name: int-1/1/1 tag: - {key: kind, value: access} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link48-leaf2-dcgw2 namespace: default spec: link: tag: endpoints: - node-name: leaf2 interface-name: int-1/1/48 tag: - {key: kind, value: access} - {key: endpoint-group, value: dcgw} - node-name: dcgw2 interface-name: int-1/1/1 tag: - {key: kind, value: access} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link1-leaf2-master0 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf2 interface-name: int-1/1/1 tag: - {key: kind, value: access} - {key: lag-name, value: lag-1} - {key: lacp-fallback, value: \"true\"} - {key: endpoint-group, value: server-pod1} - {key: multihoming, value: \"true\"} - {key: multihoming-name, value: esi1} - node-name: master0 interface-name: ens0 tag: - {key: kind, value: access} - {key: lag-name, value: bond0} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link1-leaf1-master0 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/1 tag: - {key: kind, value: access} - {key: lag-name, value: lag-1} - {key: lacp-fallback, value: \"true\"} - {key: endpoint-group, value: server-pod1} - {key: multihoming, value: \"true\"} - {key: multihoming-name, value: esi1} - node-name: master0 interface-name: ens0 tag: - {key: kind, value: access} - {key: lag-name, value: bond0} The status of the links can be shown like this k get topologylinks.topo.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ TOPO LAG MEMBER NODE-EPA ITFCE-EPA MH-EPA NODE-EPB ITFCE-EPB MH-EPB AGE nokia.region1.fabric1.link1-leaf1-master0 True True nokia region1 fabric1 true leaf1 int-1/1/1 true master0 ens0 11m nokia.region1.fabric1.link1-leaf2-master0 True True nokia region1 fabric1 true leaf2 int-1/1/1 true master0 ens0 11m nokia.region1.fabric1.link48-leaf1-dcgw1 True True nokia region1 fabric1 leaf1 int-1/1/48 dcgw1 int-1/1/1 11m nokia.region1.fabric1.link48-leaf2-dcgw2 True True nokia region1 fabric1 leaf2 int-1/1/48 dcgw2 int-1/1/1 11m nokia.region1.fabric1.link49-leaf1-leaf2 True True nokia region1 fabric1 true leaf1 int-1/1/49 leaf2 int-1/1/49 11m nokia.region1.fabric1.link50-leaf1-leaf2 True True nokia region1 fabric1 true leaf1 int-1/1/50 leaf2 int-1/1/50 11m nokia.region1.fabric1.logical-mh-link-esi1-master0 True True nokia region1 fabric1 true esi1 true master0 bond0 11m nokia.region1.fabric1.logical-sh-link-leaf1-lag-50-leaf2-lag-50 True True nokia region1 fabric1 true leaf1 lag-50 leaf2 lag-50 11m Configure the ipam registry # ipam # apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: Ipam metadata: name: nokia-default namespace: default spec: oda: - key: organization value: nokia ipam: admin-state: enable network-instance # the name structure is important to comply to odns framework apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstance metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed namespace: default spec: network-instance: admin-state: enable description: \"default network instance\" default-prefix-length: isl: address-family: ipv4: 31 ipv6: 127 loopback: address-family: ipv4: 32 ipv6: 128 prefixes for isl and looopbacks # the name structure is important to comply to odns framework apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstanceIpPrefix metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed.isl-ipv4 namespace: default spec: ip-prefix: prefix: 100.64.0.0/16 tag: - key: purpose value: isl apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstanceIpPrefix metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed.isl-ipv6 namespace: default spec: ip-prefix: prefix: 3100:100::/48 tag: - key: purpose value: isl apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstanceIpPrefix metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed.lpbk-ipv4 namespace: default spec: ip-prefix: prefix: 100.112.100.0/24 tag: - key: purpose value: loopback apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstanceIpPrefix metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed.lpbk-ipv6 namespace: default spec: ip-prefix: prefix: 2000::/64 tag: - key: purpose value: loopback Configure the ni registry # apiVersion: ni.nddr.yndd.io/v1alpha1 kind: Registry metadata: name: nokia-default namespace: default spec: oda: - key: organization value: nokia registry: description: default Network Instance pool size: 10000 show the registry status k get registries.ni.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ REGISTRY ALLOCATED AVAILABLE TOTAL AGE nokia-default True True nokia unknown unknown nokia-default 0 10000 10000 23s Configure the as registry # apiVersion: as.nddr.yndd.io/v1alpha1 kind: Registry metadata: name: nokia-default namespace: default spec: oda: - key: organization value: nokia registry: description: deterministic global AS pool allocation-strategy: deterministic start: 65100 end: 65199 show the registry status k get registries.as.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ REGISTRY STRATEGY START END ALLOCATED AVAILABLE TOTAL AGE nokia-default True True nokia unknown unknown nokia-default deterministic 65100 65199 0 100 100 2m24s","title":"Provision nddo registers"},{"location":"start/provision-nddo-registers/#provision","text":"First we need to configure the registries","title":"Provision"},{"location":"start/provision-nddo-registers/#configure-the-organization-registry","text":"","title":"Configure the organization registry"},{"location":"start/provision-nddo-registers/#organization","text":"we create an organization apiVersion: org.nddr.yndd.io/v1alpha1 kind: Organization metadata: name: nokia namespace: default spec: organization: description: default organization for Nokia register: - {kind: ipam, name: nokia-default} - {kind: ni, name: nokia-default} - {kind: as, name: nokia-default} - {kind: vlan, name: nokia-default}","title":"organization"},{"location":"start/provision-nddo-registers/#deployment","text":"create a deployment apiVersion: org.nddr.yndd.io/v1alpha1 kind: Deployment metadata: name: nokia.region1 namespace: default spec: deployment: region: an","title":"deployment"},{"location":"start/provision-nddo-registers/#configure-a-topology-in-a-deployment","text":"","title":"Configure a topology in a deployment"},{"location":"start/provision-nddo-registers/#fabric","text":"apiVersion: topo.nddr.yndd.io/v1alpha1 kind: Topology metadata: name: nokia.region1.fabric1 namespace: default spec: topology: kind: - name: srl tag: - key: platform value: ixrd2 - name: sros tag: - key: platform value: sr1 - name: linux","title":"fabric"},{"location":"start/provision-nddo-registers/#nodes","text":"apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.leaf1 namespace: default spec: node: kind-name: srl tag: - {key: index, value: \"0\"} - {key: position, value: leaf} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.leaf2 namespace: default spec: node: kind-name: srl tag: - {key: index, value: \"1\"} - {key: position, value: leaf} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.dcgw1 namespace: default spec: node: kind-name: sros tag: - {key: position, value: dcgw} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.dcgw2 namespace: default spec: node: kind-name: sros tag: - {key: position, value: dcgw} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.master0 namespace: default spec: node: kind-name: linux tag: - {key: position, value: server}","title":"nodes"},{"location":"start/provision-nddo-registers/#links","text":"apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link50-leaf1-leaf2 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/50 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} - node-name: leaf2 interface-name: int-1/1/50 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link49-leaf1-leaf2 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/49 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} - node-name: leaf2 interface-name: int-1/1/49 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link48-leaf1-dcgw1 namespace: default spec: link: tag: endpoints: - node-name: leaf1 interface-name: int-1/1/48 tag: - {key: kind, value: access} - {key: endpoint-group, value: dcgw} - node-name: dcgw1 interface-name: int-1/1/1 tag: - {key: kind, value: access} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link48-leaf2-dcgw2 namespace: default spec: link: tag: endpoints: - node-name: leaf2 interface-name: int-1/1/48 tag: - {key: kind, value: access} - {key: endpoint-group, value: dcgw} - node-name: dcgw2 interface-name: int-1/1/1 tag: - {key: kind, value: access} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link1-leaf2-master0 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf2 interface-name: int-1/1/1 tag: - {key: kind, value: access} - {key: lag-name, value: lag-1} - {key: lacp-fallback, value: \"true\"} - {key: endpoint-group, value: server-pod1} - {key: multihoming, value: \"true\"} - {key: multihoming-name, value: esi1} - node-name: master0 interface-name: ens0 tag: - {key: kind, value: access} - {key: lag-name, value: bond0} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link1-leaf1-master0 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/1 tag: - {key: kind, value: access} - {key: lag-name, value: lag-1} - {key: lacp-fallback, value: \"true\"} - {key: endpoint-group, value: server-pod1} - {key: multihoming, value: \"true\"} - {key: multihoming-name, value: esi1} - node-name: master0 interface-name: ens0 tag: - {key: kind, value: access} - {key: lag-name, value: bond0} The status of the links can be shown like this k get topologylinks.topo.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ TOPO LAG MEMBER NODE-EPA ITFCE-EPA MH-EPA NODE-EPB ITFCE-EPB MH-EPB AGE nokia.region1.fabric1.link1-leaf1-master0 True True nokia region1 fabric1 true leaf1 int-1/1/1 true master0 ens0 11m nokia.region1.fabric1.link1-leaf2-master0 True True nokia region1 fabric1 true leaf2 int-1/1/1 true master0 ens0 11m nokia.region1.fabric1.link48-leaf1-dcgw1 True True nokia region1 fabric1 leaf1 int-1/1/48 dcgw1 int-1/1/1 11m nokia.region1.fabric1.link48-leaf2-dcgw2 True True nokia region1 fabric1 leaf2 int-1/1/48 dcgw2 int-1/1/1 11m nokia.region1.fabric1.link49-leaf1-leaf2 True True nokia region1 fabric1 true leaf1 int-1/1/49 leaf2 int-1/1/49 11m nokia.region1.fabric1.link50-leaf1-leaf2 True True nokia region1 fabric1 true leaf1 int-1/1/50 leaf2 int-1/1/50 11m nokia.region1.fabric1.logical-mh-link-esi1-master0 True True nokia region1 fabric1 true esi1 true master0 bond0 11m nokia.region1.fabric1.logical-sh-link-leaf1-lag-50-leaf2-lag-50 True True nokia region1 fabric1 true leaf1 lag-50 leaf2 lag-50 11m","title":"links"},{"location":"start/provision-nddo-registers/#configure-the-ipam-registry","text":"","title":"Configure the ipam registry"},{"location":"start/provision-nddo-registers/#ipam","text":"apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: Ipam metadata: name: nokia-default namespace: default spec: oda: - key: organization value: nokia ipam: admin-state: enable","title":"ipam"},{"location":"start/provision-nddo-registers/#network-instance","text":"the name structure is important to comply to odns framework apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstance metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed namespace: default spec: network-instance: admin-state: enable description: \"default network instance\" default-prefix-length: isl: address-family: ipv4: 31 ipv6: 127 loopback: address-family: ipv4: 32 ipv6: 128","title":"network-instance"},{"location":"start/provision-nddo-registers/#prefixes-for-isl-and-looopbacks","text":"the name structure is important to comply to odns framework apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstanceIpPrefix metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed.isl-ipv4 namespace: default spec: ip-prefix: prefix: 100.64.0.0/16 tag: - key: purpose value: isl apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstanceIpPrefix metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed.isl-ipv6 namespace: default spec: ip-prefix: prefix: 3100:100::/48 tag: - key: purpose value: isl apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstanceIpPrefix metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed.lpbk-ipv4 namespace: default spec: ip-prefix: prefix: 100.112.100.0/24 tag: - key: purpose value: loopback apiVersion: ipam.nddr.yndd.io/v1alpha1 kind: IpamNetworkInstanceIpPrefix metadata: name: nokia.region1.infrastructure.infra.nokia-default.default-routed.lpbk-ipv6 namespace: default spec: ip-prefix: prefix: 2000::/64 tag: - key: purpose value: loopback","title":"prefixes for isl and looopbacks"},{"location":"start/provision-nddo-registers/#configure-the-ni-registry","text":"apiVersion: ni.nddr.yndd.io/v1alpha1 kind: Registry metadata: name: nokia-default namespace: default spec: oda: - key: organization value: nokia registry: description: default Network Instance pool size: 10000 show the registry status k get registries.ni.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ REGISTRY ALLOCATED AVAILABLE TOTAL AGE nokia-default True True nokia unknown unknown nokia-default 0 10000 10000 23s","title":"Configure the ni registry"},{"location":"start/provision-nddo-registers/#configure-the-as-registry","text":"apiVersion: as.nddr.yndd.io/v1alpha1 kind: Registry metadata: name: nokia-default namespace: default spec: oda: - key: organization value: nokia registry: description: deterministic global AS pool allocation-strategy: deterministic start: 65100 end: 65199 show the registry status k get registries.as.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ REGISTRY STRATEGY START END ALLOCATED AVAILABLE TOTAL AGE nokia-default True True nokia unknown unknown nokia-default deterministic 65100 65199 0 100 100 2m24s","title":"Configure the as registry"},{"location":"start/provision/","text":"Provision # NDD allow to provision a network device through kubernetes using a network node device (nn) driver and a ndd-provider. The network node device driver interacts with the device through GNMI and provides a caching layer between the network device and the provider. The ndd-provider installs the configuration parameters in a declaritive way to the network device, through the network node device driver. Setup a network node device driver # To setup the network node device driver we first need to configure a network node and optionally device driver parameters. Setup a secret # A secret is used to connect to the network device through gnmi. Create a nddsrlsecret.yaml file with the base64 encoded username and password file: nddsrlsecret.yaml --- apiVersion: v1 kind: Secret metadata: name: srl-secrets type: Opaque data: username: YWRtaW4K password: YWRtaW4K apply the secret to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created Create a network node file leaf1.yaml: in the credentialsName: refer to the name of the secret created in the previous step in the address: supply the IP address of the network node that was setup in the network node installation step. In this Example we use gnmi between the network device driver and the network device, so also the gnmi port is relevant in the address section. file leaf1.yaml --- apiVersion: dvr.ndd.yndd.io/v1 kind: NetworkNode metadata: name: leaf1 labels: target-group: leaf-grp1 target: leaf1 spec: # deviceDriverKind: gnmi target: address: 172.20.20.5:57400 credentialsName: srl-secrets encoding: JSON_IETF skpVerify: true apply the network node configuration to the cluster kubectl apply -f leaf1.yaml networknode.dvr.ndd.yndd.io/leaf1 unchanged After the creation of the network node we should see: a new pod appear in the cluster which is providing the caching function between the network device and the ndd-provider the network node state should be HEALTHY: True, CONFIGURED: True, READY: False. READY state is false because the provider is not yet installed and registered to the network node device driver. Pods in ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 38m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 17m ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 38m Network node status: kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True False 172.20.20.5:57400 gnmi 9999 19m Install the Provider # Install a provider which exposes the network device configuration through the kubernetes API. Create a nddsrlpackage.yaml file, which provides: - name of the provider: ndd-provider-srl - package name: yndd/ndd-provider-srl:latest file nddsrlpackage.yaml apiVersion: pkg.ndd.yndd.io/v1 kind: Provider metadata: name: ndd-provider-srl namespace: ndd-system spec: package: yndd/ndd-provider-srl:latest packagePullPolicy: Always apply the provider to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created to see the installation of the provider you can use: watch kubectl get pkg After this step is successfull you should see the provider pod running in the ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 51m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 30m ndd-provider-srl-67b9e61445f6-846cb5c64c-t2w9h 1/1 Running 0 2m35s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 51m Also you should see a number of CRDs being installed in the cluster which are owned by the provider you installed. kubectl get crd | grep srl registrations.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlbfds.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlinterfacesubinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceaggregateroutes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstancenexthopgroups.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgpevpns.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgps.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgpvpns.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsises.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolslinuxes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsospfs.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstances.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstancestaticroutes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicyaspathsets.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicycommunitysets.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicypolicies.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlroutingpolicyprefixsets.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemmtus.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnames.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsbgpvpns.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstanceesis.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstances.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpns.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemntps.srl.ndd.yndd.io 2021-09-05T12:23:02Z srltunnelinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:02Z srltunnelinterfacevxlaninterfaces.srl.ndd.yndd.io 2021-09-05T12:23:02Z When the provider and the network device type matches you should see the network node becoming in ready status and you should see that the network node discovered additional parameters of the network device, such as sw version ,type, etc kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True True 172.20.20.5:57400 gnmi nokia-srl 7220 IXR-D2 v21.3.1 02:BF:A9:FF:00:00 Sim Serial No. 9999 47m Provision a NDD managed resource # Create a managed resource for the provider installed. E.g. an interface and subinterface. These resources are specific for the ndd provider you use. A ndd managed resource has the following attributes: - Kubernetes specific parameters: - A apiversion: enabled by the provider - A Kind: enabled by the provider - Metadata: name, namespace, label and annotations - Spec specifics for ndd: - networkNodeRef: refers to the network node name that was created in the previous step - forNetworkNode: refers to the specifics of the managed resource CRD enabled by the provider: see provider API documentation for more details. file int-e1-49.yaml apiVersion: srl.ndd.yndd.io/v1 kind: SrlInterface metadata: name: int-e1-49 namespace: default spec: active: true networkNodeRef: name: leaf1 forNetworkNode: interface: name: \"ethernet-1/49\" admin-state: \"enable\" description: \"ndd-ethernet-1/49\" vlan-tagging: true kubectl apply -f int-e1-49.yaml file subint-e1-49-0.yaml apiVersion: srl.ndd.yndd.io/v1 kind: SrlInterfaceSubinterface metadata: name: subint-e1-49-0 namespace: default spec: active: true networkNodeRef: name: leaf1 forNetworkNode: interface-name: ethernet-1/49 subinterface: index: 1 type: routed admin-state: enable description: \"ndd-e1-49-0-leaf1\" ipv4: address: - ip-prefix: 100.64.0.0/31 ipv6: address: - ip-prefix: 3100:64::/127 vlan: encap: single-tagged: vlan-id: \"1\" kubectl apply -f subint-e1-49-0.yaml When loggind in to the node you should see the configuration that was supplied via the ndd-provider in the network device info from running / interface ethernet-1/49 interface ethernet-1/49 { description ndd-ethernet-1/49 admin-state enable vlan-tagging true subinterface 1 { type routed description ndd-e1-49-0-leaf1 admin-state enable ipv4 { allow-directed-broadcast false address 100.64.0.0/31 { } } ipv6 { address 3100:64::/127 { } } vlan { encap { single-tagged { vlan-id 1 } } } } }","title":"Provision a network device"},{"location":"start/provision/#provision","text":"NDD allow to provision a network device through kubernetes using a network node device (nn) driver and a ndd-provider. The network node device driver interacts with the device through GNMI and provides a caching layer between the network device and the provider. The ndd-provider installs the configuration parameters in a declaritive way to the network device, through the network node device driver.","title":"Provision"},{"location":"start/provision/#setup-a-network-node-device-driver","text":"To setup the network node device driver we first need to configure a network node and optionally device driver parameters.","title":"Setup a network node device driver"},{"location":"start/provision/#setup-a-secret","text":"A secret is used to connect to the network device through gnmi. Create a nddsrlsecret.yaml file with the base64 encoded username and password file: nddsrlsecret.yaml --- apiVersion: v1 kind: Secret metadata: name: srl-secrets type: Opaque data: username: YWRtaW4K password: YWRtaW4K apply the secret to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created Create a network node file leaf1.yaml: in the credentialsName: refer to the name of the secret created in the previous step in the address: supply the IP address of the network node that was setup in the network node installation step. In this Example we use gnmi between the network device driver and the network device, so also the gnmi port is relevant in the address section. file leaf1.yaml --- apiVersion: dvr.ndd.yndd.io/v1 kind: NetworkNode metadata: name: leaf1 labels: target-group: leaf-grp1 target: leaf1 spec: # deviceDriverKind: gnmi target: address: 172.20.20.5:57400 credentialsName: srl-secrets encoding: JSON_IETF skpVerify: true apply the network node configuration to the cluster kubectl apply -f leaf1.yaml networknode.dvr.ndd.yndd.io/leaf1 unchanged After the creation of the network node we should see: a new pod appear in the cluster which is providing the caching function between the network device and the ndd-provider the network node state should be HEALTHY: True, CONFIGURED: True, READY: False. READY state is false because the provider is not yet installed and registered to the network node device driver. Pods in ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 38m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 17m ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 38m Network node status: kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True False 172.20.20.5:57400 gnmi 9999 19m","title":"Setup a secret"},{"location":"start/provision/#install-the-provider","text":"Install a provider which exposes the network device configuration through the kubernetes API. Create a nddsrlpackage.yaml file, which provides: - name of the provider: ndd-provider-srl - package name: yndd/ndd-provider-srl:latest file nddsrlpackage.yaml apiVersion: pkg.ndd.yndd.io/v1 kind: Provider metadata: name: ndd-provider-srl namespace: ndd-system spec: package: yndd/ndd-provider-srl:latest packagePullPolicy: Always apply the provider to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created to see the installation of the provider you can use: watch kubectl get pkg After this step is successfull you should see the provider pod running in the ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 51m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 30m ndd-provider-srl-67b9e61445f6-846cb5c64c-t2w9h 1/1 Running 0 2m35s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 51m Also you should see a number of CRDs being installed in the cluster which are owned by the provider you installed. kubectl get crd | grep srl registrations.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlbfds.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlinterfacesubinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceaggregateroutes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstancenexthopgroups.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgpevpns.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgps.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsbgpvpns.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsises.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolslinuxes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstanceprotocolsospfs.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstances.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlnetworkinstancestaticroutes.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicyaspathsets.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicycommunitysets.srl.ndd.yndd.io 2021-09-05T12:23:01Z srlroutingpolicypolicies.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlroutingpolicyprefixsets.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemmtus.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnames.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsbgpvpns.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstanceesis.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstances.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemnetworkinstanceprotocolsevpns.srl.ndd.yndd.io 2021-09-05T12:23:02Z srlsystemntps.srl.ndd.yndd.io 2021-09-05T12:23:02Z srltunnelinterfaces.srl.ndd.yndd.io 2021-09-05T12:23:02Z srltunnelinterfacevxlaninterfaces.srl.ndd.yndd.io 2021-09-05T12:23:02Z When the provider and the network device type matches you should see the network node becoming in ready status and you should see that the network node discovered additional parameters of the network device, such as sw version ,type, etc kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True True 172.20.20.5:57400 gnmi nokia-srl 7220 IXR-D2 v21.3.1 02:BF:A9:FF:00:00 Sim Serial No. 9999 47m","title":"Install the Provider"},{"location":"start/provision/#provision-a-ndd-managed-resource","text":"Create a managed resource for the provider installed. E.g. an interface and subinterface. These resources are specific for the ndd provider you use. A ndd managed resource has the following attributes: - Kubernetes specific parameters: - A apiversion: enabled by the provider - A Kind: enabled by the provider - Metadata: name, namespace, label and annotations - Spec specifics for ndd: - networkNodeRef: refers to the network node name that was created in the previous step - forNetworkNode: refers to the specifics of the managed resource CRD enabled by the provider: see provider API documentation for more details. file int-e1-49.yaml apiVersion: srl.ndd.yndd.io/v1 kind: SrlInterface metadata: name: int-e1-49 namespace: default spec: active: true networkNodeRef: name: leaf1 forNetworkNode: interface: name: \"ethernet-1/49\" admin-state: \"enable\" description: \"ndd-ethernet-1/49\" vlan-tagging: true kubectl apply -f int-e1-49.yaml file subint-e1-49-0.yaml apiVersion: srl.ndd.yndd.io/v1 kind: SrlInterfaceSubinterface metadata: name: subint-e1-49-0 namespace: default spec: active: true networkNodeRef: name: leaf1 forNetworkNode: interface-name: ethernet-1/49 subinterface: index: 1 type: routed admin-state: enable description: \"ndd-e1-49-0-leaf1\" ipv4: address: - ip-prefix: 100.64.0.0/31 ipv6: address: - ip-prefix: 3100:64::/127 vlan: encap: single-tagged: vlan-id: \"1\" kubectl apply -f subint-e1-49-0.yaml When loggind in to the node you should see the configuration that was supplied via the ndd-provider in the network device info from running / interface ethernet-1/49 interface ethernet-1/49 { description ndd-ethernet-1/49 admin-state enable vlan-tagging true subinterface 1 { type routed description ndd-e1-49-0-leaf1 admin-state enable ipv4 { allow-directed-broadcast false address 100.64.0.0/31 { } } ipv6 { address 3100:64::/127 { } } vlan { encap { single-tagged { vlan-id 1 } } } } }","title":"Provision a NDD managed resource"},{"location":"start/uninstall/","text":"","title":"Uninstall ndd"}]}