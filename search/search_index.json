{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to NDD docs # NDD is an opensource Kubernetes add-on that enables platform and application teams to consume network devices in a similar way as other resources are consumed in Kubernetes . NDD uses a modular approach, through providers, which allows multiple network device types to be supported. NDD allows the network providers to be generated from YANG data models , which enables rapid enablement of multiple network device types. Through YANG we can provider automate input and dependency management between the various resource that are consumed within the device. An NDD provider represents the device model through various Customer Resources within the Kubernetes API in order to provide flexible management of the device resources. NDD is build on the basis of the kubebuilder and operator pattern within kubernetes. Features: Device discovery and Provider registration Declaritive CRUD configuration of network devices through Customer Resources Configuration Input Validation: Declarative validation using an OpenAPI v3 schema derived from YANG Runtime Dependency Management amongst the various resources comsumed within a device (parent dependency management and leaf reference dependency management amont resources) Automatic or Operator interacted configuration drift management Delete Policy, and Active etc","title":"Home"},{"location":"#welcome-to-ndd-docs","text":"NDD is an opensource Kubernetes add-on that enables platform and application teams to consume network devices in a similar way as other resources are consumed in Kubernetes . NDD uses a modular approach, through providers, which allows multiple network device types to be supported. NDD allows the network providers to be generated from YANG data models , which enables rapid enablement of multiple network device types. Through YANG we can provider automate input and dependency management between the various resource that are consumed within the device. An NDD provider represents the device model through various Customer Resources within the Kubernetes API in order to provide flexible management of the device resources. NDD is build on the basis of the kubebuilder and operator pattern within kubernetes. Features: Device discovery and Provider registration Declaritive CRUD configuration of network devices through Customer Resources Configuration Input Validation: Declarative validation using an OpenAPI v3 schema derived from YANG Runtime Dependency Management amongst the various resources comsumed within a device (parent dependency management and leaf reference dependency management amont resources) Automatic or Operator interacted configuration drift management Delete Policy, and Active etc","title":"Welcome to NDD docs"},{"location":"api/overview/","text":"API Documentation # The ndd ecosystem contains many CRDs that map to API types represented by external network device providers. The documentation for these CRDs are auto-generated on doc.crds.dev . To find the CRDs available for providers maintained by the ndd team, you can search for the Github URL, or append it in the doc.crds.dev URL path. For instance, to find the CRDs available for ndd-provider-srl , you would go to: doc.crds.dev/github.com/yndd/ndd-provider-srl By default, you will be served the latest CRDs on the master branch for the repository. If you prefer to see the CRDs for a specific version, you can append the git tag for the release: doc.crds.dev/github.com/yndd/ndd-provider-srl@v0.1.4 Ndd repositories that are not providers but do publish CRDs are also served on doc.crds.dev . For instance, the yndd/ndd-core repository. Bugs and feature requests for API documentation should be opened as issues on the open source doc.crds.dev repo .","title":"overview"},{"location":"api/overview/#api-documentation","text":"The ndd ecosystem contains many CRDs that map to API types represented by external network device providers. The documentation for these CRDs are auto-generated on doc.crds.dev . To find the CRDs available for providers maintained by the ndd team, you can search for the Github URL, or append it in the doc.crds.dev URL path. For instance, to find the CRDs available for ndd-provider-srl , you would go to: doc.crds.dev/github.com/yndd/ndd-provider-srl By default, you will be served the latest CRDs on the master branch for the repository. If you prefer to see the CRDs for a specific version, you can append the git tag for the release: doc.crds.dev/github.com/yndd/ndd-provider-srl@v0.1.4 Ndd repositories that are not providers but do publish CRDs are also served on doc.crds.dev . For instance, the yndd/ndd-core repository. Bugs and feature requests for API documentation should be opened as issues on the open source doc.crds.dev repo .","title":"API Documentation"},{"location":"community/community/","text":"Everybody is welcome to join and chat to our community members about all things ndd! Join Ndd Discord Server","title":"Community"},{"location":"concepts/architecture/","text":"","title":"Architecture"},{"location":"concepts/package/","text":"","title":"Packages"},{"location":"concepts/provider/","text":"","title":"Providers"},{"location":"concepts/resource/","text":"","title":"Managed Resource"},{"location":"concepts/terminology/","text":"","title":"Terminology"},{"location":"examples/examples/","text":"","title":"Examples"},{"location":"faq/faq/","text":"","title":"FAQ"},{"location":"start/install-nddo/","text":"Installation # NDDO is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of networking. Pre-requisites # install ndd-core Install nddo registries # There are 5 registries right now org-registry topo-registry ni-registry org-registry # create a file nddr-package-org-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-org-registry namespace: ndd-system spec: package: yndd/nddr-org-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-org-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-org-registry-ac199de081fe-5bf5bc7f96-c8sph 2/2 Running 0 12s topo-registry # create a file nddr-package-topo-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-topo-registry namespace: ndd-system spec: package: yndd/nddr-topo-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-topo-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-topo-registry-83d75d175ab9-fbcd55666-k9jtg 2/2 Running 0 7s ni-registry # create a file nddr-package-ni-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-ni-registry namespace: ndd-system spec: package: yndd/nddr-ni-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-ni-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-ni-registry-4a71a2c372d0-74589448f6-nw8hl 2/2 Running 0 7s Install nddo Intents # Install the intents, there are 2 right now: - infrastructure - Endpoint groups - Vpc nddo-infrastructure # create a file nddo-package-infra.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-infra namespace: ndd-system spec: package: yndd/nddo-intent-infra:latest packagePullPolicy: Always kubectl apply -f nddo-package-infra.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-infra-0f86fd936790-5bbdbc9f85-l7tgp 2/2 Running 0 6s nddo-epgroup # create a file nddo-package-epgroup.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-epgroup namespace: ndd-system spec: package: yndd/nddo-intent-epgroup:latest packagePullPolicy: Always kubectl apply -f nddo-package-epgroup.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-epgroup-ebcddb8968e5-77756f9fbb-jrnl4 2/2 Running 0 6s nddo-vpc # create a file nddo-package-vpc.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-vpc namespace: ndd-system spec: package: yndd/nddo-intent-vpc:latest packagePullPolicy: Always kubectl apply -f nddo-package-vpc.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-vpc-8b8d4ae0e44c-7d4bd6b589-jg49s 2/2 Running 0 6s","title":"Install nddo"},{"location":"start/install-nddo/#installation","text":"NDDO is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of networking.","title":"Installation"},{"location":"start/install-nddo/#pre-requisites","text":"install ndd-core","title":"Pre-requisites"},{"location":"start/install-nddo/#install-nddo-registries","text":"There are 5 registries right now org-registry topo-registry ni-registry","title":"Install nddo registries"},{"location":"start/install-nddo/#org-registry","text":"create a file nddr-package-org-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-org-registry namespace: ndd-system spec: package: yndd/nddr-org-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-org-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-org-registry-ac199de081fe-5bf5bc7f96-c8sph 2/2 Running 0 12s","title":"org-registry"},{"location":"start/install-nddo/#topo-registry","text":"create a file nddr-package-topo-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-topo-registry namespace: ndd-system spec: package: yndd/nddr-topo-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-topo-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-topo-registry-83d75d175ab9-fbcd55666-k9jtg 2/2 Running 0 7s","title":"topo-registry"},{"location":"start/install-nddo/#ni-registry","text":"create a file nddr-package-ni-registry.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddr-ni-registry namespace: ndd-system spec: package: yndd/nddr-ni-registry:latest packagePullPolicy: Always kubectl apply -f nddr-package-ni-registry.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddr-ni-registry-4a71a2c372d0-74589448f6-nw8hl 2/2 Running 0 7s","title":"ni-registry"},{"location":"start/install-nddo/#install-nddo-intents","text":"Install the intents, there are 2 right now: - infrastructure - Endpoint groups - Vpc","title":"Install nddo Intents"},{"location":"start/install-nddo/#nddo-infrastructure","text":"create a file nddo-package-infra.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-infra namespace: ndd-system spec: package: yndd/nddo-intent-infra:latest packagePullPolicy: Always kubectl apply -f nddo-package-infra.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-infra-0f86fd936790-5bbdbc9f85-l7tgp 2/2 Running 0 6s","title":"nddo-infrastructure"},{"location":"start/install-nddo/#nddo-epgroup","text":"create a file nddo-package-epgroup.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-epgroup namespace: ndd-system spec: package: yndd/nddo-intent-epgroup:latest packagePullPolicy: Always kubectl apply -f nddo-package-epgroup.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-epgroup-ebcddb8968e5-77756f9fbb-jrnl4 2/2 Running 0 6s","title":"nddo-epgroup"},{"location":"start/install-nddo/#nddo-vpc","text":"create a file nddo-package-vpc.yaml with the following content apiVersion: pkg.ndd.yndd.io/v1 kind: Intent metadata: name: nddo-vpc namespace: ndd-system spec: package: yndd/nddo-intent-vpc:latest packagePullPolicy: Always kubectl apply -f nddo-package-vpc.yaml after this command a pod should be create like this k get pods -n ndd-system -w NAME READY STATUS RESTARTS AGE ... nddo-vpc-8b8d4ae0e44c-7d4bd6b589-jg49s 2/2 Running 0 6s","title":"nddo-vpc"},{"location":"start/install/","text":"Installation # NDD is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of network devices. Pre-requisites # Install a Kubernetest Cluster # Install a kubernetes cluster based on your preferences. Some examples are provided here for reference, but if you would use another kubernetes cluster flavor you can skip this step. kind cluster Install the kind sw curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind Create the kind cluster kind create cluster --name ndd Check if the cluster is running kubectl get node Expected output or similar NAME STATUS ROLES AGE VERSION ndd-control-plane Ready master 2m58s v1.19.1 The full documentation for installing a kind can be found here: kind installation minikube cluster minikube getting started can be found here: https://minikube.sigs.k8s.io/docs/start/ Install a network device # Install a network device based on your preference. You can use container based labs, real HW or other options. Some examples are provided here for reference. if you have access to a network device through other mean you can skip this step. Also you can enable multiple networking devices. containerlab containerlab is a tool that deploys container based meshed lab topologies. Various vendors are supported and extensive documentation is provided here: containerlab In this example we use SRL for which containerlab sets up gnmi by default real HW Real HW installation should be looked up based on the respective vendor that is used E.g. gnmi will be used and should be configured Enable connectivity from the cluster to the network device # NDD is using outbound communication from within the kubernetes cluster to the network device. In some environments some special action need to be taken kind cluster with container lab If you run containerlab on the same linux machine and when both the kind cluster and containerlab uses seperate bridges, you should add the following rules in iptables to allow communication between the kind cluster network and container lab management network. The bridges are specific to your environment sudo iptables -I FORWARD 1 -i br-7ee0d52a222c -o br-1f1c85a8b4c5 -j ACCEPT sudo iptables -I FORWARD 1 -o br-7ee0d52a222c -i br-1f1c85a8b4c5 -j ACCEPT Install ndd-core # ndd-core provides the core of the ndd project. ndd-core enables: the installation, configuration and life cycle management of network device drivers the installation, configuration and life cycle management of ndd providers rbac for the ndd providers in the cluster latest kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml stable kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml After this step a ndd-system namespace is created, the ndd-core crds are created and 2 pods are running in ndd-system namespace. Namespace kubectl get ns NAME STATUS AGE default Active 11m kube-node-lease Active 11m kube-public Active 11m kube-system Active 11m local-path-storage Active 11m ndd-system Active 2m6s CRDs: kubectl get crd NAME CREATED AT controllerconfigs.pkg.ndd.yndd.io 2021-09-05T11:34:20Z devicedrivers.dvr.ndd.yndd.io 2021-09-05T11:34:20Z locks.pkg.ndd.yndd.io 2021-09-05T11:34:20Z networknodes.dvr.ndd.yndd.io 2021-09-05T11:34:20Z networknodeusages.dvr.ndd.yndd.io 2021-09-05T11:34:20Z providerrevisions.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.meta.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.pkg.ndd.yndd.io 2021-09-05T11:34:20Z PODs: kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 7m16s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 7m16s When all of this succeeded we can starrt provisioning network devices through kubernetes","title":"Install ndd"},{"location":"start/install/#installation","text":"NDD is an application that runs within a kubernetes cluster. Through customer resources it enables a declarative management of network devices.","title":"Installation"},{"location":"start/install/#pre-requisites","text":"","title":"Pre-requisites"},{"location":"start/install/#install-a-kubernetest-cluster","text":"Install a kubernetes cluster based on your preferences. Some examples are provided here for reference, but if you would use another kubernetes cluster flavor you can skip this step. kind cluster Install the kind sw curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64 chmod +x ./kind mv ./kind /some-dir-in-your-PATH/kind Create the kind cluster kind create cluster --name ndd Check if the cluster is running kubectl get node Expected output or similar NAME STATUS ROLES AGE VERSION ndd-control-plane Ready master 2m58s v1.19.1 The full documentation for installing a kind can be found here: kind installation minikube cluster minikube getting started can be found here: https://minikube.sigs.k8s.io/docs/start/","title":"Install a Kubernetest Cluster"},{"location":"start/install/#install-a-network-device","text":"Install a network device based on your preference. You can use container based labs, real HW or other options. Some examples are provided here for reference. if you have access to a network device through other mean you can skip this step. Also you can enable multiple networking devices. containerlab containerlab is a tool that deploys container based meshed lab topologies. Various vendors are supported and extensive documentation is provided here: containerlab In this example we use SRL for which containerlab sets up gnmi by default real HW Real HW installation should be looked up based on the respective vendor that is used E.g. gnmi will be used and should be configured","title":"Install a network device"},{"location":"start/install/#enable-connectivity-from-the-cluster-to-the-network-device","text":"NDD is using outbound communication from within the kubernetes cluster to the network device. In some environments some special action need to be taken kind cluster with container lab If you run containerlab on the same linux machine and when both the kind cluster and containerlab uses seperate bridges, you should add the following rules in iptables to allow communication between the kind cluster network and container lab management network. The bridges are specific to your environment sudo iptables -I FORWARD 1 -i br-7ee0d52a222c -o br-1f1c85a8b4c5 -j ACCEPT sudo iptables -I FORWARD 1 -o br-7ee0d52a222c -i br-1f1c85a8b4c5 -j ACCEPT","title":"Enable connectivity from the cluster to the network device"},{"location":"start/install/#install-ndd-core","text":"ndd-core provides the core of the ndd project. ndd-core enables: the installation, configuration and life cycle management of network device drivers the installation, configuration and life cycle management of ndd providers rbac for the ndd providers in the cluster latest kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml stable kubectl apply -f https://raw.githubusercontent.com/yndd/ndd-core/master/config/manifests.yaml After this step a ndd-system namespace is created, the ndd-core crds are created and 2 pods are running in ndd-system namespace. Namespace kubectl get ns NAME STATUS AGE default Active 11m kube-node-lease Active 11m kube-public Active 11m kube-system Active 11m local-path-storage Active 11m ndd-system Active 2m6s CRDs: kubectl get crd NAME CREATED AT controllerconfigs.pkg.ndd.yndd.io 2021-09-05T11:34:20Z devicedrivers.dvr.ndd.yndd.io 2021-09-05T11:34:20Z locks.pkg.ndd.yndd.io 2021-09-05T11:34:20Z networknodes.dvr.ndd.yndd.io 2021-09-05T11:34:20Z networknodeusages.dvr.ndd.yndd.io 2021-09-05T11:34:20Z providerrevisions.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.meta.pkg.ndd.yndd.io 2021-09-05T11:34:20Z providers.pkg.ndd.yndd.io 2021-09-05T11:34:20Z PODs: kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 7m16s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 7m16s When all of this succeeded we can starrt provisioning network devices through kubernetes","title":"Install ndd-core"},{"location":"start/provision-nddo-intents/","text":"Provision # After the registers are setup we can configure the intents infrastructure # apiVersion: infra.nddo.yndd.io/v1alpha1 kind: Infrastructure metadata: name: nokia.region1.infra namespace: default spec: infrastructure: network-instance-name: default as: 65555 as-pool: start: 65000 end: 65400 cidr: loopback-cidr-ipv4: \"100.64.0.0/24\" loopback-cidr-ipv6: \"1000:64::/64\" admin-state: enable addressing-scheme: dual-stack underlay-protocol: - 'ebgp' overlay-protocol: - 'evpn' show status k get infrastructures.infra.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ NI ADDR UNDERLAY OVERLAY AGE nokia.region1.infra True True nokia region1 default-routed dual-stack ebgp evpn 23s this should result in the network instances and interfaces created in the nddp-layer k get srl2 endpoint groups # server pods apiVersion: epg.nddo.yndd.io/v1alpha1 kind: EpGroup metadata: name: nokia.region1.server-pod1 namespace: default spec: ep-group: admin-state: enable dcgw apiVersion: epg.nddo.yndd.io/v1alpha1 kind: EpGroup metadata: name: nokia.region1.dcgw namespace: default spec: ep-group: admin-state: enable show status k get epgroups.epg.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ EPGNAME DEPL AGE nokia.region1.dcgw True True nokia region1 dcgw 72s nokia.region1.server-pod1 True True nokia region1 server-pod1 72s This shoudl result in additional interfaces created in the adaptor layer k get srl2 vpc # apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.prov namespace: default spec: vpc: admin-state: enable description: vpc for server provisioning bridge-domains: - name: provisioning interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} outer-vlan-id: 10 apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.infra namespace: default spec: vpc: defaults: tunnel: vxlan protocol: evpn admin-state: enable description: vpc for server infrastructure bridge-domains: - name: infrastructure tunnel: vxlan protocol: evpn interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} outer-vlan-id: 40 routing-tables: - name: infrastructure tunnel: vxlan protocol: evpn bridge-domains: - name: infrastructure ipv4-prefixes: [100.112.3.0/24] ipv6-prefixes: [2a02:1800:80:7000::/64] interface-selector: - tag: - {key: kind, value: node-itfce} - {key: leaf1, value: int-1/1/48} ipv4-prefixes: [100.112.10.1/31] ipv6-prefixes: [2a02:1800:80:7050::1/64] outer-vlan-id: 10 - tag: - {key: kind, value: node-itfce} - {key: leaf2, value: int-1/1/48} ipv4-prefixes: [100.112.10.3/31] ipv6-prefixes: [2a02:1800:80:7051::1/64] outer-vlan-id: 11 apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.multus namespace: default spec: vpc: defaults: tunnel: vxlan protocol: evpn admin-state: enable description: vpc for multus bridge-domains: - name: multus-ipvlan interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} - {key: itfce-kind, value: ipvlan} outer-vlan-id: 100 - name: multus-sriov4 interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} - {key: itfce-kind, value: sriov1} outer-vlan-id: 200 - name: multus-sriov55 interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} - {key: itfce-kind, value: sriov2} outer-vlan-id: 102 routing-tables: - name: multus bridge-domains: - name: multus-ipvlan2 ipv4-prefixes: [100.112.1.3/24] ipv6-prefixes: [2a02:1800:81:8000::/64] - name: multus-sriov12 ipv4-prefixes: [100.112.2.0/24] ipv6-prefixes: [2a02:1800:82:7000::/64] - name: multus-sriov2 ipv4-prefixes: [100.112.3.0/24] ipv6-prefixes: [2a02:1800:83:7000::/64] interface-selector: - tag: - {key: kind, value: node-itfce} - {key: leaf1, value: int-1/1/48} ipv4-prefixes: [100.112.10.1/31] ipv6-prefixes: [2a02:1800:80:7050::1/64] outer-vlan-id: 100 - tag: - {key: kind, value: node-itfce} - {key: leaf2, value: int-1/1/48} ipv4-prefixes: [100.112.10.3/31] ipv6-prefixes: [2a02:1800:80:7050::3/64] outer-vlan-id: 110","title":"Provision nddo intents"},{"location":"start/provision-nddo-intents/#provision","text":"After the registers are setup we can configure the intents","title":"Provision"},{"location":"start/provision-nddo-intents/#infrastructure","text":"apiVersion: infra.nddo.yndd.io/v1alpha1 kind: Infrastructure metadata: name: nokia.region1.infra namespace: default spec: infrastructure: network-instance-name: default as: 65555 as-pool: start: 65000 end: 65400 cidr: loopback-cidr-ipv4: \"100.64.0.0/24\" loopback-cidr-ipv6: \"1000:64::/64\" admin-state: enable addressing-scheme: dual-stack underlay-protocol: - 'ebgp' overlay-protocol: - 'evpn' show status k get infrastructures.infra.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ NI ADDR UNDERLAY OVERLAY AGE nokia.region1.infra True True nokia region1 default-routed dual-stack ebgp evpn 23s this should result in the network instances and interfaces created in the nddp-layer k get srl2","title":"infrastructure"},{"location":"start/provision-nddo-intents/#endpoint-groups","text":"server pods apiVersion: epg.nddo.yndd.io/v1alpha1 kind: EpGroup metadata: name: nokia.region1.server-pod1 namespace: default spec: ep-group: admin-state: enable dcgw apiVersion: epg.nddo.yndd.io/v1alpha1 kind: EpGroup metadata: name: nokia.region1.dcgw namespace: default spec: ep-group: admin-state: enable show status k get epgroups.epg.nddo.yndd.io NAME SYNC STATUS ORG DEP AZ EPGNAME DEPL AGE nokia.region1.dcgw True True nokia region1 dcgw 72s nokia.region1.server-pod1 True True nokia region1 server-pod1 72s This shoudl result in additional interfaces created in the adaptor layer k get srl2","title":"endpoint groups"},{"location":"start/provision-nddo-intents/#vpc","text":"apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.prov namespace: default spec: vpc: admin-state: enable description: vpc for server provisioning bridge-domains: - name: provisioning interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} outer-vlan-id: 10 apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.infra namespace: default spec: vpc: defaults: tunnel: vxlan protocol: evpn admin-state: enable description: vpc for server infrastructure bridge-domains: - name: infrastructure tunnel: vxlan protocol: evpn interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} outer-vlan-id: 40 routing-tables: - name: infrastructure tunnel: vxlan protocol: evpn bridge-domains: - name: infrastructure ipv4-prefixes: [100.112.3.0/24] ipv6-prefixes: [2a02:1800:80:7000::/64] interface-selector: - tag: - {key: kind, value: node-itfce} - {key: leaf1, value: int-1/1/48} ipv4-prefixes: [100.112.10.1/31] ipv6-prefixes: [2a02:1800:80:7050::1/64] outer-vlan-id: 10 - tag: - {key: kind, value: node-itfce} - {key: leaf2, value: int-1/1/48} ipv4-prefixes: [100.112.10.3/31] ipv6-prefixes: [2a02:1800:80:7051::1/64] outer-vlan-id: 11 apiVersion: vpc.nddo.yndd.io/v1alpha1 kind: Vpc metadata: name: nokia.region1.multus namespace: default spec: vpc: defaults: tunnel: vxlan protocol: evpn admin-state: enable description: vpc for multus bridge-domains: - name: multus-ipvlan interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} - {key: itfce-kind, value: ipvlan} outer-vlan-id: 100 - name: multus-sriov4 interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} - {key: itfce-kind, value: sriov1} outer-vlan-id: 200 - name: multus-sriov55 interface-selector: - tag: - {key: kind, value: epg} - {key: endpoint-group, value: server-pod1} - {key: itfce-kind, value: sriov2} outer-vlan-id: 102 routing-tables: - name: multus bridge-domains: - name: multus-ipvlan2 ipv4-prefixes: [100.112.1.3/24] ipv6-prefixes: [2a02:1800:81:8000::/64] - name: multus-sriov12 ipv4-prefixes: [100.112.2.0/24] ipv6-prefixes: [2a02:1800:82:7000::/64] - name: multus-sriov2 ipv4-prefixes: [100.112.3.0/24] ipv6-prefixes: [2a02:1800:83:7000::/64] interface-selector: - tag: - {key: kind, value: node-itfce} - {key: leaf1, value: int-1/1/48} ipv4-prefixes: [100.112.10.1/31] ipv6-prefixes: [2a02:1800:80:7050::1/64] outer-vlan-id: 100 - tag: - {key: kind, value: node-itfce} - {key: leaf2, value: int-1/1/48} ipv4-prefixes: [100.112.10.3/31] ipv6-prefixes: [2a02:1800:80:7050::3/64] outer-vlan-id: 110","title":"vpc"},{"location":"start/provision-nddo-registers/","text":"Provision # First we need to configure the registries Configure the organization registry # organization # we create an organization apiVersion: org.nddr.yndd.io/v1alpha1 kind: Organization metadata: name: nokia namespace: default spec: organization: description: default organization for Nokia register: - {kind: ipam, name: nokia-default} - {kind: ni, name: nokia-default} - {kind: as, name: nokia-default} - {kind: vlan, name: nokia-default} deployment # create a deployment apiVersion: org.nddr.yndd.io/v1alpha1 kind: Deployment metadata: name: nokia.region1 namespace: default spec: deployment: region: an Configure a topology in a deployment # fabric # apiVersion: topo.nddr.yndd.io/v1alpha1 kind: Topology metadata: name: nokia.region1.fabric1 namespace: default spec: topology: kind: - name: srl tag: - key: platform value: ixrd2 - name: sros tag: - key: platform value: sr1 - name: linux nodes # apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.leaf1 namespace: default spec: node: kind-name: srl tag: - {key: index, value: \"0\"} - {key: position, value: leaf} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.leaf2 namespace: default spec: node: kind-name: srl tag: - {key: index, value: \"1\"} - {key: position, value: leaf} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.dcgw1 namespace: default spec: node: kind-name: sros tag: - {key: position, value: dcgw} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.dcgw2 namespace: default spec: node: kind-name: sros tag: - {key: position, value: dcgw} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.master0 namespace: default spec: node: kind-name: linux tag: - {key: position, value: server} links # apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link50-leaf1-leaf2 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/50 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} - node-name: leaf2 interface-name: int-1/1/50 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link49-leaf1-leaf2 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/49 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} - node-name: leaf2 interface-name: int-1/1/49 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link48-leaf1-dcgw1 namespace: default spec: link: tag: endpoints: - node-name: leaf1 interface-name: int-1/1/48 tag: - {key: kind, value: access} - {key: endpoint-group, value: dcgw} - node-name: dcgw1 interface-name: int-1/1/1 tag: - {key: kind, value: access} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link48-leaf2-dcgw2 namespace: default spec: link: tag: endpoints: - node-name: leaf2 interface-name: int-1/1/48 tag: - {key: kind, value: access} - {key: endpoint-group, value: dcgw} - node-name: dcgw2 interface-name: int-1/1/1 tag: - {key: kind, value: access} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link1-leaf2-master0 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf2 interface-name: int-1/1/1 tag: - {key: kind, value: access} - {key: lag-name, value: lag-1} - {key: lacp-fallback, value: \"true\"} - {key: endpoint-group, value: server-pod1} - {key: multihoming, value: \"true\"} - {key: multihoming-name, value: esi1} - node-name: master0 interface-name: ens0 tag: - {key: kind, value: access} - {key: lag-name, value: bond0} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link1-leaf1-master0 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/1 tag: - {key: kind, value: access} - {key: lag-name, value: lag-1} - {key: lacp-fallback, value: \"true\"} - {key: endpoint-group, value: server-pod1} - {key: multihoming, value: \"true\"} - {key: multihoming-name, value: esi1} - node-name: master0 interface-name: ens0 tag: - {key: kind, value: access} - {key: lag-name, value: bond0} The status of the links can be shown like this k get topologylinks.topo.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ TOPO LAG MEMBER NODE-EPA ITFCE-EPA MH-EPA NODE-EPB ITFCE-EPB MH-EPB AGE nokia.region1.fabric1.link1-leaf1-master0 True True nokia region1 fabric1 true leaf1 int-1/1/1 true master0 ens0 11m nokia.region1.fabric1.link1-leaf2-master0 True True nokia region1 fabric1 true leaf2 int-1/1/1 true master0 ens0 11m nokia.region1.fabric1.link48-leaf1-dcgw1 True True nokia region1 fabric1 leaf1 int-1/1/48 dcgw1 int-1/1/1 11m nokia.region1.fabric1.link48-leaf2-dcgw2 True True nokia region1 fabric1 leaf2 int-1/1/48 dcgw2 int-1/1/1 11m nokia.region1.fabric1.link49-leaf1-leaf2 True True nokia region1 fabric1 true leaf1 int-1/1/49 leaf2 int-1/1/49 11m nokia.region1.fabric1.link50-leaf1-leaf2 True True nokia region1 fabric1 true leaf1 int-1/1/50 leaf2 int-1/1/50 11m nokia.region1.fabric1.logical-mh-link-esi1-master0 True True nokia region1 fabric1 true esi1 true master0 bond0 11m nokia.region1.fabric1.logical-sh-link-leaf1-lag-50-leaf2-lag-50 True True nokia region1 fabric1 true leaf1 lag-50 leaf2 lag-50 11m Configure the ni registry # The ni registry helps to allocate unique identifiers for the vpc when using irb constructs and are used for vxlan and irb identifiers apiVersion: ni.nddr.yndd.io/v1alpha1 kind: Registry metadata: name: nokia-default namespace: default spec: oda: - key: organization value: nokia registry: description: default Network Instance pool size: 10000 show the registry status k get registries.ni.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ REGISTRY ALLOCATED AVAILABLE TOTAL AGE nokia-default True True nokia unknown unknown nokia-default 0 10000 10000 23s","title":"Provision nddo registers"},{"location":"start/provision-nddo-registers/#provision","text":"First we need to configure the registries","title":"Provision"},{"location":"start/provision-nddo-registers/#configure-the-organization-registry","text":"","title":"Configure the organization registry"},{"location":"start/provision-nddo-registers/#organization","text":"we create an organization apiVersion: org.nddr.yndd.io/v1alpha1 kind: Organization metadata: name: nokia namespace: default spec: organization: description: default organization for Nokia register: - {kind: ipam, name: nokia-default} - {kind: ni, name: nokia-default} - {kind: as, name: nokia-default} - {kind: vlan, name: nokia-default}","title":"organization"},{"location":"start/provision-nddo-registers/#deployment","text":"create a deployment apiVersion: org.nddr.yndd.io/v1alpha1 kind: Deployment metadata: name: nokia.region1 namespace: default spec: deployment: region: an","title":"deployment"},{"location":"start/provision-nddo-registers/#configure-a-topology-in-a-deployment","text":"","title":"Configure a topology in a deployment"},{"location":"start/provision-nddo-registers/#fabric","text":"apiVersion: topo.nddr.yndd.io/v1alpha1 kind: Topology metadata: name: nokia.region1.fabric1 namespace: default spec: topology: kind: - name: srl tag: - key: platform value: ixrd2 - name: sros tag: - key: platform value: sr1 - name: linux","title":"fabric"},{"location":"start/provision-nddo-registers/#nodes","text":"apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.leaf1 namespace: default spec: node: kind-name: srl tag: - {key: index, value: \"0\"} - {key: position, value: leaf} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.leaf2 namespace: default spec: node: kind-name: srl tag: - {key: index, value: \"1\"} - {key: position, value: leaf} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.dcgw1 namespace: default spec: node: kind-name: sros tag: - {key: position, value: dcgw} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.dcgw2 namespace: default spec: node: kind-name: sros tag: - {key: position, value: dcgw} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyNode metadata: name: nokia.region1.fabric1.master0 namespace: default spec: node: kind-name: linux tag: - {key: position, value: server}","title":"nodes"},{"location":"start/provision-nddo-registers/#links","text":"apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link50-leaf1-leaf2 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/50 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} - node-name: leaf2 interface-name: int-1/1/50 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link49-leaf1-leaf2 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/49 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} - node-name: leaf2 interface-name: int-1/1/49 tag: - {key: kind, value: infra} - {key: lag-name, value: lag-50} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link48-leaf1-dcgw1 namespace: default spec: link: tag: endpoints: - node-name: leaf1 interface-name: int-1/1/48 tag: - {key: kind, value: access} - {key: endpoint-group, value: dcgw} - node-name: dcgw1 interface-name: int-1/1/1 tag: - {key: kind, value: access} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link48-leaf2-dcgw2 namespace: default spec: link: tag: endpoints: - node-name: leaf2 interface-name: int-1/1/48 tag: - {key: kind, value: access} - {key: endpoint-group, value: dcgw} - node-name: dcgw2 interface-name: int-1/1/1 tag: - {key: kind, value: access} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link1-leaf2-master0 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf2 interface-name: int-1/1/1 tag: - {key: kind, value: access} - {key: lag-name, value: lag-1} - {key: lacp-fallback, value: \"true\"} - {key: endpoint-group, value: server-pod1} - {key: multihoming, value: \"true\"} - {key: multihoming-name, value: esi1} - node-name: master0 interface-name: ens0 tag: - {key: kind, value: access} - {key: lag-name, value: bond0} apiVersion: topo.nddr.yndd.io/v1alpha1 kind: TopologyLink metadata: name: nokia.region1.fabric1.link1-leaf1-master0 namespace: default spec: link: tag: - {key: lag-member, value: \"true\"} endpoints: - node-name: leaf1 interface-name: int-1/1/1 tag: - {key: kind, value: access} - {key: lag-name, value: lag-1} - {key: lacp-fallback, value: \"true\"} - {key: endpoint-group, value: server-pod1} - {key: multihoming, value: \"true\"} - {key: multihoming-name, value: esi1} - node-name: master0 interface-name: ens0 tag: - {key: kind, value: access} - {key: lag-name, value: bond0} The status of the links can be shown like this k get topologylinks.topo.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ TOPO LAG MEMBER NODE-EPA ITFCE-EPA MH-EPA NODE-EPB ITFCE-EPB MH-EPB AGE nokia.region1.fabric1.link1-leaf1-master0 True True nokia region1 fabric1 true leaf1 int-1/1/1 true master0 ens0 11m nokia.region1.fabric1.link1-leaf2-master0 True True nokia region1 fabric1 true leaf2 int-1/1/1 true master0 ens0 11m nokia.region1.fabric1.link48-leaf1-dcgw1 True True nokia region1 fabric1 leaf1 int-1/1/48 dcgw1 int-1/1/1 11m nokia.region1.fabric1.link48-leaf2-dcgw2 True True nokia region1 fabric1 leaf2 int-1/1/48 dcgw2 int-1/1/1 11m nokia.region1.fabric1.link49-leaf1-leaf2 True True nokia region1 fabric1 true leaf1 int-1/1/49 leaf2 int-1/1/49 11m nokia.region1.fabric1.link50-leaf1-leaf2 True True nokia region1 fabric1 true leaf1 int-1/1/50 leaf2 int-1/1/50 11m nokia.region1.fabric1.logical-mh-link-esi1-master0 True True nokia region1 fabric1 true esi1 true master0 bond0 11m nokia.region1.fabric1.logical-sh-link-leaf1-lag-50-leaf2-lag-50 True True nokia region1 fabric1 true leaf1 lag-50 leaf2 lag-50 11m","title":"links"},{"location":"start/provision-nddo-registers/#configure-the-ni-registry","text":"The ni registry helps to allocate unique identifiers for the vpc when using irb constructs and are used for vxlan and irb identifiers apiVersion: ni.nddr.yndd.io/v1alpha1 kind: Registry metadata: name: nokia-default namespace: default spec: oda: - key: organization value: nokia registry: description: default Network Instance pool size: 10000 show the registry status k get registries.ni.nddr.yndd.io NAME SYNC STATUS ORG DEP AZ REGISTRY ALLOCATED AVAILABLE TOTAL AGE nokia-default True True nokia unknown unknown nokia-default 0 10000 10000 23s","title":"Configure the ni registry"},{"location":"start/provision/","text":"Provision # NDD allow to provision a network device through kubernetes using a provider. The provider interacts with the device through GNMI and implements a caching layer for robust operations between kubernetes and the network device. The provider installs the configuration parameters in a declaritive way to the network device and ensure the k8s crds are synchronized with the device. Install the Provider # Install a provider which exposes the network device configuration through the kubernetes API. Create a nddp-package-srl.yaml file, which provides: - name of the provider: nddp-srl - package name: yndd/nddp-srl:latest file nddp-package-srl.yaml apiVersion: pkg.ndd.yndd.io/v1 kind: Provider metadata: name: nddp-srl namespace: ndd-system spec: package: yndd/nddp-srl:latest packagePullPolicy: Always apply the provider to the cluster to see the installation of the provider you can use: watch kubectl get pkg After this step is successfull you should see the provider pod running in the ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 51m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 30m nddp-srl-51ef00fd88c2-98fb57465-lm5td 2/2 Running 0 4m3s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 51m Also you should see a number of CRDs being installed in the cluster which are owned by the provider you installed. kubectl get crd | grep srl srlbfds.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlinterfaces.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlinterfacesubinterfaces.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceaggregateroutes.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstancenexthopgroups.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsbgpevpns.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsbgps.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsbgpvpns.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsises.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolslinuxes.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsospfs.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstances.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstancestaticroutes.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlroutingpolicyaspathsets.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlroutingpolicycommunitysets.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlroutingpolicypolicies.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlroutingpolicyprefixsets.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnames.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnetworkinstanceprotocolsbgpvpns.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstanceesis.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstances.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnetworkinstanceprotocolsevpns.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemntps.srl.nddp.yndd.io 2022-02-09T14:48:29Z srltransactions.srl.nddp.yndd.io 2022-02-13T20:38:49Z srltunnelinterfaces.srl.nddp.yndd.io 2022-02-09T14:48:29Z srltunnelinterfacevxlaninterfaces.srl.nddp.yndd.io 2022-02-09T14:48:29Z Setup a network node device driver # To setup the network node device we first need to configure a network node. Setup a secret # A secret is used to connect to the network device through gnmi. Create a nddsrlsecret.yaml file with the base64 encoded username and password file: nddsrlsecret.yaml --- apiVersion: v1 kind: Secret metadata: name: srl-secrets type: Opaque data: username: YWRtaW4K password: YWRtaW4K apply the secret to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created Create a network node file leaf1.yaml: in the credentialsName: refer to the name of the secret created in the previous step in the address: supply the IP address of the network node that was setup in the network node installation step. In this Example we use gnmi between the network device driver and the network device, so also the gnmi port is relevant in the address section. file leaf1.yaml --- apiVersion: dvr.ndd.yndd.io/v1 kind: NetworkNode metadata: name: leaf1 labels: target-group: leaf-grp1 target: leaf1 spec: # deviceDriverKind: gnmi target: address: 172.20.20.5:57400 credentialsName: srl-secrets encoding: JSON_IETF skpVerify: true apply the network node configuration to the cluster kubectl apply -f leaf1.yaml networknode.dvr.ndd.yndd.io/leaf1 unchanged When the provider and the network device type matches you should see the network node becoming in ready status and you should see that the network node got discovered with additional parameters of the network device, such as sw version ,type, etc kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True True 172.20.20.5:57400 gnmi nokia-srl 7220 IXR-D2 v21.3.1 02:BF:A9:FF:00:00 Sim Serial No. 9999 47m Provision a NDD managed resource # Create a managed resource for the provider installed. E.g. an interface and subinterface. These resources are specific for the ndd provider you use. A ndd managed resource has the following attributes: - Kubernetes specific parameters: - A apiversion: enabled by the provider - A Kind: enabled by the provider - Metadata: name, namespace, label and annotations - Spec specifics for ndd: - networkNodeRef: refers to the network node name that was created in the previous step - forNetworkNode: refers to the specifics of the managed resource CRD enabled by the provider: see provider API documentation for more details. file int-e1-49.yaml <<<<<<< HEAD apiVersion: srl.ndd.yndd.io/v1alpha1 ======= apiVersion: srl.nddp.yndd.io/v1alpha1 >>>>>>> d896e49 (added some new docs) kind: SrlInterface metadata: name: int-e1-49 namespace: default spec: active: true networkNodeRef: name: leaf1 interface: name: \"ethernet-1/49\" admin-state: \"enable\" description: \"ndd-ethernet-1/49\" vlan-tagging: true kubectl apply -f int-e1-49.yaml file subint-e1-49-0.yaml apiVersion: srl.nddp.yndd.io/v1alpha1 kind: SrlInterfaceSubinterface metadata: name: subint-e1-49-0 namespace: default spec: active: true networkNodeRef: name: leaf1 interface-name: ethernet-1/49 subinterface: index: 1 type: routed admin-state: enable description: \"ndd-e1-49-0-leaf1\" ipv4: address: - ip-prefix: 100.64.0.0/31 ipv6: address: - ip-prefix: 3100:64::/127 vlan: encap: single-tagged: vlan-id: \"1\" kubectl apply -f subint-e1-49-0.yaml When loggind in to the node you should see the configuration that was supplied via the ndd-provider in the network device info from running / interface ethernet-1/49 interface ethernet-1/49 { description ndd-ethernet-1/49 admin-state enable vlan-tagging true subinterface 1 { type routed description ndd-e1-49-0-leaf1 admin-state enable ipv4 { allow-directed-broadcast false address 100.64.0.0/31 { } } ipv6 { address 3100:64::/127 { } } vlan { encap { single-tagged { vlan-id 1 } } } } }","title":"Provision a network device"},{"location":"start/provision/#provision","text":"NDD allow to provision a network device through kubernetes using a provider. The provider interacts with the device through GNMI and implements a caching layer for robust operations between kubernetes and the network device. The provider installs the configuration parameters in a declaritive way to the network device and ensure the k8s crds are synchronized with the device.","title":"Provision"},{"location":"start/provision/#install-the-provider","text":"Install a provider which exposes the network device configuration through the kubernetes API. Create a nddp-package-srl.yaml file, which provides: - name of the provider: nddp-srl - package name: yndd/nddp-srl:latest file nddp-package-srl.yaml apiVersion: pkg.ndd.yndd.io/v1 kind: Provider metadata: name: nddp-srl namespace: ndd-system spec: package: yndd/nddp-srl:latest packagePullPolicy: Always apply the provider to the cluster to see the installation of the provider you can use: watch kubectl get pkg After this step is successfull you should see the provider pod running in the ndd-system namespace kubectl get pods -n ndd-system NAME READY STATUS RESTARTS AGE ndd-core-5dd5b5c679-6hkvw 2/2 Running 0 51m ndd-dep-leaf1-5bf7b6b89f-ncd8x 1/1 Running 0 30m nddp-srl-51ef00fd88c2-98fb57465-lm5td 2/2 Running 0 4m3s ndd-rbac-6d8d68dbbc-58rnt 2/2 Running 0 51m Also you should see a number of CRDs being installed in the cluster which are owned by the provider you installed. kubectl get crd | grep srl srlbfds.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlinterfaces.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlinterfacesubinterfaces.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceaggregateroutes.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstancenexthopgroups.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsbgpevpns.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsbgps.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsbgpvpns.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsises.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolslinuxes.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstanceprotocolsospfs.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstances.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlnetworkinstancestaticroutes.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlroutingpolicyaspathsets.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlroutingpolicycommunitysets.srl.nddp.yndd.io 2022-02-09T14:48:28Z srlroutingpolicypolicies.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlroutingpolicyprefixsets.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnames.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnetworkinstanceprotocolsbgpvpns.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstanceesis.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnetworkinstanceprotocolsevpnesisbgpinstances.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemnetworkinstanceprotocolsevpns.srl.nddp.yndd.io 2022-02-09T14:48:29Z srlsystemntps.srl.nddp.yndd.io 2022-02-09T14:48:29Z srltransactions.srl.nddp.yndd.io 2022-02-13T20:38:49Z srltunnelinterfaces.srl.nddp.yndd.io 2022-02-09T14:48:29Z srltunnelinterfacevxlaninterfaces.srl.nddp.yndd.io 2022-02-09T14:48:29Z","title":"Install the Provider"},{"location":"start/provision/#setup-a-network-node-device-driver","text":"To setup the network node device we first need to configure a network node.","title":"Setup a network node device driver"},{"location":"start/provision/#setup-a-secret","text":"A secret is used to connect to the network device through gnmi. Create a nddsrlsecret.yaml file with the base64 encoded username and password file: nddsrlsecret.yaml --- apiVersion: v1 kind: Secret metadata: name: srl-secrets type: Opaque data: username: YWRtaW4K password: YWRtaW4K apply the secret to the cluster kubectl apply -f nddsrlsecret.yaml secret/srl-secrets created Create a network node file leaf1.yaml: in the credentialsName: refer to the name of the secret created in the previous step in the address: supply the IP address of the network node that was setup in the network node installation step. In this Example we use gnmi between the network device driver and the network device, so also the gnmi port is relevant in the address section. file leaf1.yaml --- apiVersion: dvr.ndd.yndd.io/v1 kind: NetworkNode metadata: name: leaf1 labels: target-group: leaf-grp1 target: leaf1 spec: # deviceDriverKind: gnmi target: address: 172.20.20.5:57400 credentialsName: srl-secrets encoding: JSON_IETF skpVerify: true apply the network node configuration to the cluster kubectl apply -f leaf1.yaml networknode.dvr.ndd.yndd.io/leaf1 unchanged When the provider and the network device type matches you should see the network node becoming in ready status and you should see that the network node got discovered with additional parameters of the network device, such as sw version ,type, etc kubectl get nn NAME HEALTHY CONFIGURED READY ADDRESS CONN-KIND TYPE KIND SWVERSION MACADDRESS SERIALNBR GRPCSERVERPORT AGE leaf1 True True True 172.20.20.5:57400 gnmi nokia-srl 7220 IXR-D2 v21.3.1 02:BF:A9:FF:00:00 Sim Serial No. 9999 47m","title":"Setup a secret"},{"location":"start/provision/#provision-a-ndd-managed-resource","text":"Create a managed resource for the provider installed. E.g. an interface and subinterface. These resources are specific for the ndd provider you use. A ndd managed resource has the following attributes: - Kubernetes specific parameters: - A apiversion: enabled by the provider - A Kind: enabled by the provider - Metadata: name, namespace, label and annotations - Spec specifics for ndd: - networkNodeRef: refers to the network node name that was created in the previous step - forNetworkNode: refers to the specifics of the managed resource CRD enabled by the provider: see provider API documentation for more details. file int-e1-49.yaml <<<<<<< HEAD apiVersion: srl.ndd.yndd.io/v1alpha1 ======= apiVersion: srl.nddp.yndd.io/v1alpha1 >>>>>>> d896e49 (added some new docs) kind: SrlInterface metadata: name: int-e1-49 namespace: default spec: active: true networkNodeRef: name: leaf1 interface: name: \"ethernet-1/49\" admin-state: \"enable\" description: \"ndd-ethernet-1/49\" vlan-tagging: true kubectl apply -f int-e1-49.yaml file subint-e1-49-0.yaml apiVersion: srl.nddp.yndd.io/v1alpha1 kind: SrlInterfaceSubinterface metadata: name: subint-e1-49-0 namespace: default spec: active: true networkNodeRef: name: leaf1 interface-name: ethernet-1/49 subinterface: index: 1 type: routed admin-state: enable description: \"ndd-e1-49-0-leaf1\" ipv4: address: - ip-prefix: 100.64.0.0/31 ipv6: address: - ip-prefix: 3100:64::/127 vlan: encap: single-tagged: vlan-id: \"1\" kubectl apply -f subint-e1-49-0.yaml When loggind in to the node you should see the configuration that was supplied via the ndd-provider in the network device info from running / interface ethernet-1/49 interface ethernet-1/49 { description ndd-ethernet-1/49 admin-state enable vlan-tagging true subinterface 1 { type routed description ndd-e1-49-0-leaf1 admin-state enable ipv4 { allow-directed-broadcast false address 100.64.0.0/31 { } } ipv6 { address 3100:64::/127 { } } vlan { encap { single-tagged { vlan-id 1 } } } } }","title":"Provision a NDD managed resource"},{"location":"start/uninstall/","text":"","title":"Uninstall ndd"}]}